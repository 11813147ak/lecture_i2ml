<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(ggplot2)
library(randomForest)
library(colorspace)

library(mlr)
library(ElemStatLearn) # for spam data set
library(mboost)
library(mlbench)

@

<<size = "scriptsize", include=FALSE>>=
source("rsrc/cim1_optim.R")
@

<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Gradient Boosting}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Gradient descent}
% \begin{vbframe}{Forward stagewise additive modeling}

% Let's recall gradient descent from numerical optimization.
Let $\risk(\thetab)$ be (just for the next few slides) an arbitrary, differentiable, unconstrained objective function, which we want to minimize. The gradient $\nabla \risk(\thetab)$ is the direction of the steepest ascent, $-\nabla \risk(\thetab)$ of \textbf{steepest descent}.

\lz

For an intermediate solution $\thetab^{[k]}$ during minimization, we can iteratively improve by updating
$$
\thetab^{[k+1]} = \thetab^{[k]} - \beta \cdot \nabla \risk(\thetab^{[k]}) \qquad  \text{for } 0 < \beta \leq c\left(\thetab^{[k]}\right)
$$
% \enquote{Walking down the hill, towards the valley.}

% $f(x_1, x_2) = -\sin(0.8 x_1) \cdot \frac{1}{2\pi} \exp\left( (x_2 x_1 + \pi / 2)^2 \right)$
<<sd-plot>>=
#modified from ../../../cim1/2017/11-Optimierung/functions.

sd_plot = function(col = terrain_hcl, theta = 40, phi = 40, xlab = "x", ylab = "y") {
  if (is.function(col)) col = col(nrow(z) * ncol(z))

  par(mfrow = c(1, 2))

  par(mar = rep(0.5, 4))
  require("colorspace")
  pmat = persp2(x, y, z, theta = theta, phi = phi, ticktype = "detailed",
      xlab = xlab, ylab = ylab, zlab = "", col = col, lwd = 0.3, border = NA)

  for (j in seq_along(p)) {
    t3d = trans3d(p[[j]][[1]], p[[j]][[2]], do.call(foo, p[[j]]), pmat)
    if (j > 1) {
      t3d2 = trans3d(p[[j - 1]][[1]], p[[j - 1]][[2]], do.call(foo, p[[j- 1]]), pmat)
      lines(c(t3d$x, t3d2$x), c(t3d$y, t3d2$y))
      points(x = t3d2$x, y = t3d2$y, pch = 16, col = heat_hcl(1))
    }
    points(x = t3d$x, y = t3d$y, pch = 16, col = heat_hcl(1))
  }
  par(mar = c(4.1, 4.1, 1.1, 1.1))
  image(x, y, z, col = col, xlab = xlab, ylab = ylab, useRaster = TRUE)
  contour(x, y, z, add = TRUE, nlevels = 15)
  for (j in seq_along(p)) {
    if (j > 1) {
      lines(c(p[[j]][1], p[[j - 1]][1]), c(p[[j]][2], p[[j - 1]][2]))
      points(p[[j - 1]][1], p[[j - 1]][2], pch = 16, col = heat_hcl(1))
    }
    points(p[[j]][1], p[[j]][2], pch = 16, col = heat_hcl(1))
  }
  invisible(NULL)
}

@
<<gradient-descent, fig.align="center", echo=FALSE, fig.width=8, fig.height=4, out.height="3cm", out.width="6cm">>=
foo = function(x, y) {
  -1 * sin(.8 * pi*x) * dnorm(-y * x, mean = pi / 2, sd = 0.8)
}

x = seq(0, 2.5, length = 50)
y = seq(-3, 1, length = 50)
z = outer(x, y, foo)
p = c(list(list(1.8, -.5)), optim0(1.8, -.5, FUN = foo, maximum = FALSE, maxit = 19))

sd_plot(phi = 35, theta = -20, xlab = "x_1", ylab = "x_2", col = viridis::viridis)
@

% {\scriptsize step size  $\beta = 1$}

% \framebreak

% $\beta$ is called \textbf{step size}, and can be set by
% \begin{itemize}
% \item fixing it to a (smallish) constant
% \item adapting it according to previous gradient values, the local Hessian, etc.
% \item line search methods, which solve $\beta^{[k]} = \argmin_{\beta} f\left(x^{[k]} - \beta \nabla f\left(x^{[k]}\right)\right)$. Only one real parameter $\beta$, i.e, \enquote{easy} to solve\dots
% \end{itemize}




\end{vbframe}


\begin{vbframe}{Forward stagewise additive modeling}

Assume a regression problem for now (as this is simpler to explain);
and assume a space of base learners $\mathcal{B}$.

\lz

% A weak learner should have the property that it delivers better predictions than by random chance (e.g. for a balanced training set a misclassification error less than 1/2).

% \lz

We want to learn an additive model:

$$
\fx = \sum_{m=1}^M \betam b(\xb, \thetam)
$$

Hence, we minimize the empirical risk:

$$
\riskef = \sum_{i=1}^n L\left(\yi,\fxi \right) =
\sum_{i=1}^n L\left(\yi, \sum_{m=1}^M \betam b\left(\xi, \thetam\right)\right)
$$


% \framebreak

% A common \textbf{loss} for \textbf{regression} is the \textbf{squared error} with
% $\Lxy = (y-\fx)^2$.

% \lz

% Apparently, $\risk$ depends on the \textbf{base learners} $b(x, \thetam)$,
% or rather their parameters $\thetam$ and weights $\betam$. Hence, we have to optimize these.

% \lz

\framebreak

Because of the additive structure, it is difficult to jointly minimize $\riskef$ w.r.t. $\left(\left(\beta^{[1]}, \thetab^{[1]}\right), \ldots, \left(\beta^{[M]}, \thetab^{[M]}\right)\right)$, which is a very high-dimensional parameter space.

\lz

Moreover, considering trees as base learners is worse, as we would now have to grow $M$ trees in parallel so they
  work optimally together as an ensemble.

\lz

Finally, stagewise additive modeling has nice properties, which we want to make use of, e.g. for regularization, early stopping, \dots

\framebreak

Hence, we add additive components in a greedy fashion by sequentially minimizing the risk only w.r.t. the next additive component:

$$ \min \limits_{\beta, \thetab} \sum_{i=1}^n L\left(\yi, \fmdh\left(\xi\right) + \beta \cdot b\left(\xi, \thetab\right)\right) $$

\lz

Doing this iteratively is called \textbf{forward stagewise additive modeling}

\input{algorithms/forward_stagewise_additive_modeling.tex}

\end{vbframe}



\begin{vbframe}{Gradient boosting}

The algorithm we just introduced isn't really an algorithm, but rather an abstract principle.
We need to find the new additive component $b\left(x, \thetam\right)$ and its
weight coefficient $\betam$ in each iteration $m$.
This can be done by gradient descent, but in function space.

\lz

Thought experiment:
Consider a completely non-parametric model $f$,
where we can arbitrarily define its predictions on every point of the training data $\xi$. So we basically specify
$f$ as a discrete, finite vector.

  $$\left(f(\xb^{(1)}), \ldots,  f(\xb^{(n)})\right)^\top $$

This implies $n$ parameters $\fxi$ (and the model would provide no generalization...).

Furthermore, we assume our loss function $L$ to be differentiable.


\framebreak

Now we want to minimize the risk of such a model with gradient descent (yes, this makes no sense,
suspend all doubts for a few seconds).

So, we calculate the gradient at a point of the parameter space, that is the derivative with respect to each component of the parameter vector.

$$
  \fp{\riske}{\fxi} = \fp{\sum_j L(y^{(j)}, f(x^{(j)}))}{\fxi} = \fp{\Lxyi}{\fxi}
$$

The gradient descent update for each vector component of $f$ is:

$$
  \fxi \leftarrow \fxi - \beta \fp{\Lxyi}{\fxi}
$$

This tells us how we could \enquote{nudge} our whole function $f$ in the direction of the data to
reduce its empirical risk.


\framebreak

Combining this with the iterative additive procedure
of \enquote{forward stagewise modelling}, we are at the spot $\fmd$ during minimization.
At this point, we calculate the direction of the negative gradient:

$$ \rmi = -\left[\fp{\Lxyi}{f(\xi)}\right]_{f=\fmd} $$

The pseudo residuals $\rmi$ match the usual residuals for the squared loss:


$$
- \fp{\Lxy}{\fx} = - \fp{0.5(y - \fx)^2}{\fx} = y - \fx
$$


\framebreak

% We find our $\betam$ by minimizing with line search:

% $$
  % \betam = \argmin_{\beta} \sumin L(\yi, \fmdh(x) + \beta b(x, \thetamh)),
% $$

% where $h(x, \thetam) = \rmm$.

% \lz

What is the point in doing all this? A model parameterized in this way is senseless,
as it is just memorizing the instances of the training data...?

\vspace*{0.3cm}


So, we restrict our additive components to $b\left(\xb, \thetam\right) \in \mathcal{B}$.

% \framebreak

The pseudo-residuals are calculated exactly as stated above,
then we fit a regression model $b(\xb, \thetam)$ to them:
$$ \thetamh = \argmin_{\thetab} \sum_{i=1}^n (\rmi - b(\xi, \thetab))^2 $$
So, evaluated on the training data,
our $b(\xb, \thetam)$ corresponds as closely as possible to the negative
loss function gradient and generalizes to the whole space.

\vspace*{0.3cm}

\textbf{In a nutshell}: One boosting iteration is exactly one approximated gradient step in function space,
which minimizes the empirical risk as much as possible.


\framebreak


<<gbm-illu-plot, echo=FALSE, results="asis", out.width="0.9\\textwidth", fig.width=7, fig.height=6>>=
source("rsrc/boosting_illustration_plot.R")

set.seed(123)
x = sort(runif(100, 0, 18))
y = sin(x / 2) + rnorm(length(x), 0, sd = 0.2)

plotBoostingIllustration(x, y, nboost = 8, learning_rate = 0.2, basis_fun = bTrafo, plot_frames = c(2))
@

\end{vbframe}
%% Combining this with the iterative additive procedure
%% of \enquote{forward stagewise modelling}, we are at the spot $\fmd$ during minimization.
%% At this point, we now calculate the direction of the negative gradient:
%%
%% $$ \rmi = -\left[\fp{\Lxyi}{f(\xi)}\right]_{f=\fmd} $$
%%
%% We will call these $\rmi$ \textbf{pseudo residuals}. For squared loss they match the usual residuals
%%
%%
%% $$
%% - \fp{\Lxy}{\fx} = - \fp{0.5(y - \fx)^2}{\fx} = y - \fx
%% $$
%%
%%
%% \framebreak
%%
%% % We find our $\betam$ by minimizing with line search:
%%
%% % $$
%%   % \betam = \argmin_{\beta} \sumin L(\yi, \fmdh(x) + \beta b(x, \thetamh)),
%% % $$
%%
%% % where $h(x, \thetam) = \rmm$.
%%
%% % \lz
%%
%% What is the point in doing all this? A model parameterized in this way is senseless,
%% as it is just memorizing the instances of the training data...?
%%
%% \lz
%%
%% So, we restrict our additive components to $b\left(x, \thetam\right) \in \mathcal{B}$.
%%
%% % \framebreak
%%
%% The pseudo-residuals are calculated exactly as stated above,
%% then we fit a regression model $b(\bm{x}, \thetam)$ to them:
%% $$ \thetamh = \argmin_{\thetab} \sum_{i=1}^n (\rmi - b(\xi, \thetab))^2 $$
%% So, evaluated on the training data,
%% our $b(x, \thetam)$ corresponds as closely as possible to the negative
%% loss function gradient and generalizes to the whole space.
%%
%% \lz
%%
%% \textbf{In a nutshell}: One boosting iteration is exactly one approximated gradient step in function space,
%% which minimizes the empirical risk as much as possible.
%%
%% \end{vbframe}

\begin{vbframe}{Gradient boosting algorithm}

\input{algorithms/gradient_boosting_general.tex}

Note that we also initialize the model in a loss-optimal manner.

\end{vbframe}

\begin{vbframe}{Gradient boosting illustration - 1}


Assume one feature $x$ and a target $y$.
<<echo=FALSE, out.width="0.4\\textwidth", fig.width=4, fig.height=3>>=
nsim = 50L

set.seed(31415)
x = seq(0, 10, length.out = nsim)
y = 4 + 3 * x + 5 * sin(x) + rnorm(nsim, 0, 2)

plot(x = x, y = y, xlab = "Feature x", ylab = "Target y")
@

\begin{enumerate}

  \item
    We start with the simplest model, the optimal constant w.r.t. the $L_2$ loss (mean of the target variable).

  \item
    To improve the model we calculate the pointwise residuals on the training data $\yi - f\left(\xi\right)$ and fit a GAM fitted on the residuals.

  \item
    The GAM fitted on the residuals is then multiplied by a learning rate of $0.2$ and added to the previous model.

  \item
    This procedure is repeated multiple times.

\end{enumerate}

\end{vbframe}


<<echo=FALSE, results="asis", out.width="0.8\\textwidth", fig.width=7, fig.height=6>>=
source("rsrc/boosting_intro_animation.R")

basisTrafo = function (x) {
  margin = 0.1 * (max(x) - min(x))
  splines::splineDesign(knots = seq(min(x) - margin, max(x) + margin, length.out = 40), x = x)
}
boosting_iters = c(1,2,3,5,10,100)

for (nboost in boosting_iters) {
  cat("\\begin{vbframe}{Gradient boosting illustration - 1}\n")
  plotLineaBoosting(x, y, nboost, 0.2, basis_fun = basisTrafo)
  if (nboost > 1)
    cat("\\addtocounter{framenumber}{-1}\n\n")
  cat("\\end{vbframe}\n\n")
}
@

\begin{vbframe}{Gradient boosting illustration - 2}

We will consider a regression problem on the following slides:

\begin{itemize}
  \item
    We draw points from a sine-function with some additional noise.

  \item
    We use squared loss for $L$.

  \item
    Our base learner are tree stumps.

  \item
    The left plot shows the additive model learnt so far with the data, the right plot shows the residuals to which we fit the next base learner.

  \item
    The plot is generated by a self-implemented version of boosting, where the step size is obtained by line search.

  %\item Quiz question: Why is the obtained optimal step size always 1?

\end{itemize}

\end{vbframe}

<<gbm-anim-prep, include = FALSE>>=
source("rsrc/gbm_anim.R")

set.seed(122)
n = 50L
x = sort(10 * runif(n))
X = data.frame(x1 = x)
y = sin(x) + rnorm(length(x), mean = 0, sd = 0.04)

z = anim(X = X, y = y, M = 100, demo = FALSE, data.all.iterations = TRUE)

plotGBMIteration = function(m) {
  return(plotModel(m - 1, X[,1L], yhat = z$traces[[m]]$yhat, r = z$traces[[m]]$r,
    rhat = z$traces[[m]]$rhat, beta = z$betas[[m]]))
}
@

<<gbm-anim-plot, echo=FALSE, results="asis">>=
for (nboost in c(1:11, 31)) {
  cat("\\begin{vbframe}{Gradient boosting illustration - 2}\n")
  plotAnimation(plotGBMIteration, nboost)
  if (nboost > 1)
    cat("\\addtocounter{framenumber}{-1}\n\n")
  cat("\\end{vbframe}\n\n")
}
@

\begin{vbframe}{Gradient boosting illustration - 2}

<<gbm-sine-plot, fig.height=4>>=
source("rsrc/gbm_sine.R")
p1 = doPlot(sd = 0.0, shrinkage = 1) + theme(legend.position = "none")
p2 = doPlot(sd = 0.1, shrinkage = 1)
grid.arrange(p1, p2, nrow = 1, widths = c(0.25, 0.3))
@
\begin{itemize}

  \item
    Iterating this very simple base learner yields a rather nice approximation of a smooth model in the end.

  \item
    Severe overfitting apparent in the noisy case. We'll discuss and solve this problem later.

\end{itemize}

\end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm1.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm2.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm3.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm4.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm5.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm6.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm7.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm8.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm9.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm10.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}


\begin{vbframe}{Gradient Boosting Playground}
\begin{center}

\includegraphics[width=0.7\textwidth]{figure_man/gbm_playground.png}

\href{http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}

\begin{vbframe}{mlrPlayground}
\begin{center}

\includegraphics[width=\linewidth]{figure_man/mlrplayground_welcome.png}

\href{https://compstat-lmu.shinyapps.io/mlrPlayground/}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}
% \begin{vbframe}{RF vs AdaBoost vs GBM}

% Recall the Spirals data from mlbench. We are using stumps again. Performance is measured with 3 times repeated 10CV.

%<<fig.height=4, eval = FALSE, include = FALSE>>=
% load("rsrc/comparing_methods.RData")

%ggplot(data = ggd2[ggd2$learner %in% c("rf", "ada", "gbm", "rf.fulltree"), ],
%  aes(x = iters, y = mmce, col = learner)) + geom_line()
%@

% \end{vbframe}


\endlecture