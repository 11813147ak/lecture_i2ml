<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(ggplot2)
library(randomForest)
library(colorspace)

library(mlr)
library(ElemStatLearn) # for spam data set
library(mboost)
library(mlbench)

@

<<size = "scriptsize", include=FALSE>>=
source("rsrc/cim1_optim.R")
@

<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Gradient Boosting: Regularization}
\lecture{Introduction to Machine Learning}


\section{Regularization and Shrinkage}

\begin{vbframe}{Regularization and shrinkage}

If GB runs for a long number of iterations, it can overfit due to its aggressive loss
minimization.

\begin{blocki}{Options for regularization}
\item Limit the number of boosting iterations $M$ (\enquote{early stopping}), i.e., limit the number of additive components.
\item Limit the depth of the trees. This can also be interpreted as choosing the order of interaction.
\item Shorten the step length $\betam$ in each iteration.
\end{blocki}

The latter is achieved by multiplying $\betam$ with a small $\nu \in (0,1]$:
$$ \fm(\xb) = \fmd(\xb) + \nu \cdot \betam \cdot b(\xb, \thetam) $$

$\nu$ is called \textbf{shrinkage parameter} or \textbf{learning rate}.


\framebreak

Obviously, the optimal values for $M$ and $\nu$ strongly depend on each other:
By increasing $M$ one can use a smaller value for $\nu$ and vice versa.

\lz

In practice it is often recommended to choose $\nu$ quite small and choose $M$ by cross-validation.

\lz

It's probably best to tune all three parameters jointly based on the training data
via cross-validation or a related method.

\end{vbframe}

\begin{vbframe}{Stochastic gradient boosting}

This is a minor modification to boosting to incorporate the advantages of bagging into the method.
The idea was formulated quite early by Breiman.

\lz

Instead of fitting on all the data points, a random subsample is drawn in each iteration.

\lz

Especially for small training sets, this simple modification often leads to
significant empirical improvements.
How large the improvements are depends on data structure, size of the data set,
base learner and size of the subsamples (so this is another tuning parameter).


\end{vbframe}


%\begin{vbframe}{Variable importance}

%As for random forests, we can construct a variable importance measure to help
%interpret the model.

%\lz
%For a regression tree $b$, we can define such a measure $I^2_j(b)$ as the sum over the reduction of
%impurity ($\rightarrow$ reduction of variance) at all inner knots where the tree splits with respect to feature $x_j$.

%\framebreak

%For an additive boosting model one just takes

%$$ I^2_j = \frac{1}{M} \sum_{m=1}^M I^2_j(\bmm) $$

%%From those importance values we take the squared root and scale them so that the most important feature gets the value 100.
%To get the relative influence measures, the resulting $I^2_j, j = 1, \dots, p$ are scaled so that they sum to 1.

%\lz
%For a $g$-class problem one has $g$

%$$ \fxk = \sum_{m=1}^M b_k^{[m]}(x), \quad
%  I^2_{jk} = \frac{1}{M} \sum_{m=1}^M I^2_j(b_k^{[m]}) $$

%$$ I^2_j = \frac{1}{g} \sum_{k=1}^g I^2_{jk} $$

%\end{vbframe}


\begin{vbframe}{Example: Spam detection}

% The data set we will examine briefly in the following was collected at the Hewlett Packard laboratories
% to train a personalized spam mail detector.

% \lz

% It contains data of 4601 emails. 2788 mails were regular mails and 1813 were spam.
% There are 57 numerical predictors available measuring e.g. the frequency of the most frequent words
% and special characters as well as runlengths of words in all capitals.

% \lz

% We use the R package \pkg{gbm}, which implements the introduced version of gradient boosting.

% \framebreak

% <<gbm-spam-example, eval = FALSE, echo = TRUE>>=
% library(gbm)
% data(spam, package = "ElemStatLearn")
% spam$spam = as.numeric(spam$spam) - 1 # gbm requires target to be 0/1
% gbm(spam ~ ., data = spam,
%   distribution = "bernoulli", # classification
%   n.trees = 100, # M = 100
%   interaction.depth = 2, # max. tree depth = 2
%   shrinkage = 0.001, # nu = 0.001
% )
% @
%
% \lz

We fit a gradient boosting model for different parameter values

\begin{table}[]
\centering
\begin{tabular}{l|c}
Parameter name      & Values                         \\
\hline
distribution        & Bernoulli (for classification) \\
shrinkage $\nu$     & ${0.001, 0.01, 0.1}$           \\
number of trees $M$ & $[0,\dots,20000]$              \\
max. tree depth     & ${1,4,7,10,13,16}$
\end{tabular}
\end{table}

We observe the error on a separate test set to find the optimal parameters.


\framebreak

Misclassification rate for different hyperparameter settings (shrinkage and maximum tree depth) of gradient boosting:

<<gbm-spam-results, fig.height=4.5>>=
load("rsrc/gbm_spam_results_long.RData")

ggd = error.df
ggd$shrinkage = as.factor(ggd$shrinkage)
ggplot(data = ggd, aes(x = M, y = err, col = max.tree.depth)) +
  geom_line(alpha= .8) + facet_grid(~ shrinkage, labeller = label_both) + #coord_cartesian(ylim = c(0, 0.20)) +
  ylab("mmce (test data)") + scale_x_continuous("iterations M [1000]",
    c(1, 5, 10, 15, 20) * 1e3,
    labels = c("1", "5", "10", "15", "20")) +
  scale_y_continuous(trans = "log2")
@

% \begin{figure}
%   \includegraphics[width=10cm, height=6cm]{figure_man/gbm_spam_effects.pdf}
% \end{figure}

% \framebreak

% \begin{figure}
  % \includegraphics[width=8cm]{figure_man/gbm_spam_imp_ggplot.pdf}
  % \caption{\footnotesize Variable Importance for model with $\nu = 0.1, M = 1380$ and tree depth of $4$.}
% \end{figure}

% \framebreak
<<include=FALSE, eval=FALSE>>=
gbm.perf(mod$learner.model, method = "OOB")
@

% \begin{figure}
%  \includegraphics[width=6cm]{figure_man/gbm_spam_gbmperf.pdf}
%  \caption{\footnotesize Deviance}
% \end{figure}

% \framebreak
<<include=FALSE, eval=FALSE>>=
plot.gbm(mod$learner.model, 52)
plot.gbm(mod$learner.model, 25)
@
%
% \begin{figure}
%   \includegraphics[width=8cm, height=5cm]{figure_man/gbm_spam_partdep.pdf}
%   \caption{\footnotesize Partial Dependency Plot for 2 important features.
%   Plotted is f in dependency of one feature, if all other features are integrated over.}
% \end{figure}

\end{vbframe}



\endlecture
