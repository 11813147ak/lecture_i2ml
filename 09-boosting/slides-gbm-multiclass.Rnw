<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(ggplot2)
library(randomForest)
library(colorspace)

library(mlr)
library(ElemStatLearn) # for spam data set
library(mboost)
library(mlbench)

@

<<size = "scriptsize", include=FALSE>>=
source("rsrc/cim1_optim.R")
@

<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Gradient Boosting: Other Problem Classes}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Multi-class problems}

We proceed as in softmax regression and model a categorical distribution, with multinomial loss.
For $\Yspace = \{1, \ldots, g\}$, we create $g$ discriminant functions $\fxk$, one for each class, each an additive model of trees.

We define the $\pi_k(\xb)$ through the softmax function:
$$ \pikx = s_k(f_1(\xb), \ldots, f_g(\xb)) = \exp(\fxk) / \sum_{j=1}^g \exp(f_j(\xb)) $$

Multinomial loss $L$:
$$ L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx $$

And for the derivative the following holds:
$$-\fp{L(y_k, f_1(\xb), \ldots, f_g(\xb))}{\fxk} =  \I(y = k) - \pikx $$

\framebreak

\lz

Determining the tree structure by squared-error-loss works just like before in the 2 class problem.

\lz

In the estimation of the $c$ though, all the models depend on each other because of the definition
of $L$. Optimizing this is more difficult, so we will skip the details and just present the results.

\lz

The estimated class for $\xb$ is of course exactly the $k$ for which $\pikx$ is maximal.

\framebreak

\input{algorithms/gradient_boosting_for_k_classification.tex}

\framebreak

\begin{blocki}{Derivation of the algorithm:}

  \item from Friedman, J. H. - Greedy Function Approximation: A Gradient Boosting Machine (1999)

  \item In each iteration $m$ we calculate the pseudo residuals
        $$\rmi_k = \I(\yi = k) - \pi_k^{[m-1]}(\xi),$$
        where $\pi_k^{[m-1]}(\xi)$ is derived from $f^{[m-1]}(\mathbf{x})$

  \item Thus, $g$ trees are induced at each iteration $m$ to predict the corresponding current pseudo residuals for each class on the probability scale.

  \item Each of these trees has $T$ terminal nodes with corresponding regions $R_{tk}^{[m]}$.

\end{blocki}

\framebreak

\begin{itemize}

  \item The model updates $\hat{c}_{tk}^{[m]}$ corresponding to these regions are the solution to

  $$ \hat{c}_{tk}^{[m]} = \argmin_{c} \sum_{i=1}^n \sum_{k=1}^g L \left( \yi_k, f^{[m-1]}(\mathbf{x}^{(i)}) + \sum_{t=1}^T \hat{c}_{tk} \I\left(\xi \in R_{t}^{[m]}\right) \right)$$

  where $L$ is the multinomial loss function $L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx$
  and $\pikx = \frac{\exp(f_k(\xb))}{\sum_j \exp(f_j(\xb))}$ as before.

  \item This has no closed form solution and additionally, the regions corresponding to the different class tress overlap, so that the solution does not reduce to a separate calculation within each region of each tree.

\end{itemize}

\framebreak

\begin{itemize}

  \item Hence, we approximate the solution with a single Newton-Raphson step, using a diagonal approximation to the Hessian.

  \item This decomposes the problem into a separate calculation for each terminal node of each tree.

  \item The result is

  $$\hat{c}_{tk}^{[m]} =
      \frac{g-1}{g}\frac{\sum_{\xi \in R_{tk}^{[m]}} \rmi_k}{\sum_{\xi \in R_{tk}^{[m]}} \left|\rmi_k\right|\left(1 - \left|\rmi_k\right|\right)}$$

  \item The update is then done by
  $$\hat{f}_k^{[m]}(x) = \hat{f}_k^{[m-1]}(x) + \sum_t \hat{c}_{tk}^{[m]} \I\left(x \in R_{tk}^{[m]}\right)$$

\end{itemize}

\end{vbframe}


\begin{vbframe}{Additional information}

By choosing a suitable loss function it is also possible to model a large number of different problem domains
\begin{itemize}
  \item Regression
  \item (Multi-class) Classification
  \item Count data
  \item Survival data
  \item Ordinal data
  \item Quantile regression
  \item Ranking problems
  \item ...
\end{itemize}

\lz

% Boosting is closely related to L1 regularization.

% \lz

Different base learners increase flexibility (see componentwise gradient boosting).
If we model only individual variables, the resulting regularized variable selection
is closely related to L1 regularization.

\framebreak

For example, using the pinball loss in boosting
$$
L(y, f(\xv)) = \left\{
\begin{array}{lc}
(1 - \alpha)(f(\xv) - y), & \text{if}\ y < f(\xv) \\
\alpha(y - f(\xv)),       & \text{if}\ y \geq f(\xv)
\end{array}
\right.
$$
models the $\alpha$-quantiles:

\begin{center}
\includegraphics[scale=0.5]{figure_man/quantile_boosting.png}
\end{center}

\framebreak

The AdaBoost fit has the structure of an additive model with \enquote{basis functions} $\bmm (\xb)$.

\lz

It can be shown (see Hastie et al. 2009, chapter 10) that AdaBoost corresponds to minimizing the empirical risk in each iteration $m$ using the {\em exponential} loss function:
\begin{align*}
  L(y, \fmh(\mathbf{x}))    &= \exp\left(-y\fmh(\mathbf{x})\right) \\
  \riske(\fmh)              &= \sumin L(\yi, \fmh(\xi)) \\
                            &= \sumin L(\yi, \fmdh(\xi) + \beta b(\xi))
\end{align*}


% \begin{align*}
%   \sum_{i=1}^n \exp\left(-\yi \cdot \left(\beta b\left(\xi\right)
%   + \fmdh\left(\xi\right)\right)\right),
% \end{align*}
with minimizing over $\beta$ and $b$ and where $\fmdh$ is the boosting fit in iteration $m-1$.

% \framebreak

% AdaBoost is the empirical equivalent to the forward piecewise solution of the minimization problem

% \begin{align*}
%   \text{arg} \min_{f} \E_{y|x}( \exp (- y \cdot \fx))\ .
% \end{align*}

% \lz

% Therefore, the boosting fit is an estimate of function
% \begin{align*}
%   f^*(x) = 0.5 \cdot \log \left( \frac{\text{P} (y = 1 | x)}
%   {\text{P} (y = -1 | x)}\right) \ ,
% \end{align*}
% which solves the former problem theoretically.

% \lz

% Obvious idea: generalization on other loss functions, use of alternative basis methods.

\end{vbframe}


\begin{vbframe}{Take home message}
Gradient boosting is a statistical reinterpretation of the older AdaBoost algorithm.

\lz

Base learners are added in a \enquote{greedy} fashion, so that they point in the direction of the negative gradient of the empirical risk.

\lz

Regression base learners are fitted even for classification problems.

\lz

Often the base learners are (shallow) trees, but arbitrary base learners are possible.

\lz

The method can be adjusted flexibly by changing the loss function, as long as it's differentiable.

\lz

Methods to evaluate variable importance and to do variable selection exist.

\end{vbframe}


\endlecture