
\begin{frame}{Clustering: Customer Segmentation}

\begin{itemize}
  \item In marketing, customer segmentation is an important task to understand customer needs and to meet with customer expectations.
  \item Customer data is partitioned in terms of similiarities and the characteristics of each group are summarized.
  \item Marketing strategies are designed and prioritized according to the group size.
\end{itemize}

\lz
Example Use Cases:

\begin{itemize}
  \item Personalized ads (e.g., recommend articles).
  \item Music/Movie recommendation systems.
\end{itemize}

\end{frame}

<!-- # ML Use Cases: Clustering -->

<!-- - Initially define number of groups that are expected to have.  -->

<!-- - Each data point is assigned to cluster centers and new cluster centers are calculated. -->

<!-- - Iterative process is completed when the cluster centers converge/no significant change in cluster centers observed.  -->


## Clustering: Image Compression

```{r, include = FALSE, echo = FALSE, message = FALSE} 
ap = function (myfile) paste0(file.path(readSlide("clustering_use_case", file = FALSE), "/figure", myfile))
```

- An image consists of pixels arranged in rows and columns.
- Each pixel contains **RGB** color information, i.e., a mix of the intensity of 3 **primary colors**: **R**ed, **G**reen and **B**lue.
- Each primary color takes intensity values between 0 and 255.

\begin{center}
\includegraphics[width=0.45\textwidth]{`r ap("rgb-cube.png")`}

\tiny Source: By Ferlixwangg \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0}, from \href{https://commons.wikimedia.org/wiki/File:Rgb-cube.gif}{Wikimedia Commons}.
\end{center}


## Clustering: Image Compression

An image can be compressed by reducing its color information, i.e., by replacing similar colors of each pixel with, say, $k$ distinct colors.

**Example**: 

```{r, echo = FALSE, fig.height=5, fig.width=10}
library(jpeg)

img = readJPEG(ap("colorful_bird.jpg")) # Read the image
imgDm = dim(img)
# We re-arranged the 3D array into a dataset that has the coordinates of the pixel and the color information (R, G and B)
imgRGB = data.frame(
  x = rep(1:imgDm[2], each = imgDm[1]),
  y = rep(imgDm[1]:1, imgDm[2]),
  R = as.vector(img[,,1]),
  G = as.vector(img[,,2]),
  B = as.vector(img[,,3])
)

par(mfrow = c(1,2), mar = c(0,0,2,0))
# We now plot the pixels (pch = 15) with its RGB color info
plot(imgRGB$x, imgRGB$y, col = rgb(imgRGB[c("R", "G", "B")]), pch = 15, axes = F, main = "Original Image", xlab = "", ylab = "")
box()

k = 16
kMeans = kmeans(imgRGB[, c("R", "G", "B")], centers = k)
predRGB = kMeans$centers[kMeans$cluster,]

plot(imgRGB$x, imgRGB$y, col = rgb(predRGB), pch = 15, axes = F, main = "Image using 16 Colors", xlab = "", ylab = "")
box()
```










## Dimensionality Reduction Task

**Goal**: Describe data with fewer features (reduce number of columns). \
$\Rightarrow$ there will always be an information loss.

\begin{center}
\begin{tabular}{ | c | c | c | c | c | c |}
    \hline
      & & & & & \\ \hline
      & & & & & \\ \hline
      & & & & & \\
    \hline
  \end{tabular} $\Rightarrow$
    \begin{tabular}{ | c | c | c |}
    \hline
      & &  \\ \hline
      & &  \\ \hline
      & &  \\
    \hline
  \end{tabular}
\end{center}

- Unsupervised Methods:
    - Principle Component Analysis (PCA).
    - Factor Analysis (FA).
    - Feature filter methods.

- Supervised Methods:
    - Linear Discriminant Analysis (LDA).
    - Feature filter methods.




# Principal Component Analysis (PCA)








## Normalizing Data

A variable $X$ can be normalized by substracting its values with the mean $\bar{X}$ and dividing by the standard deviation $s_X$, e.g. $\tilde{X} = \tfrac{X - \bar{X}}{s_X}.$

**Example**:
```{r, echo = FALSE, results='hide'}
bh = t(data.frame(body.height = c("Person A" = 180, "Person B" = 172, "Person C" = 175)))
```

Consider the following body heights measured in different units:

```{r, echo=FALSE, results='asis'}
bh.m = bh/100
bh.feet = round(bh/30.48, 4)

options(xtable.comment = FALSE)
tab = rbind(cbind(bh, "mean" = mean(bh), "sd" = sd(bh)), 
  cbind(bh.m, "mean" = mean(bh.m), "sd" = sd(bh.m)),
  cbind(bh.feet, "mean" = mean(bh.feet), "sd" = sd(bh.feet)))
row.names(tab) = c("body height (cm)", "body height (m)", "body height (feet)")

xtable(tab, align = c("c|", "c", "c", "c", "|c", "c"))
```

After normalizing, we always obtain the normalized body height (no matter which unit was used):

```{r, echo = FALSE, results='asis'}
bh.norm = (bh - mean(bh))/sd(bh)
bh.norm = cbind(bh.norm, "mean" = round(mean(bh.norm), 0), "sd" = sd(bh.norm))
row.names(bh.norm) = "normalized body height"
xtable(bh.norm, align = c("c|", "c", "c", "c", "|c", "c"))
```

## Normalizing Data

Normalizing all variables in a data set, can have several advantages:

- It puts all variables into *comparable* units, i.e., we make sure that all normalized variables have mean 0 and standard deviation of 1.
- It can avoid numerical instabilites in several algorithms, e.g. if a variable has very low / high values.
- It helps in computing meaningful *distances* between observations.

## Normalizing Data: Distances

```{r, include = FALSE, echo = FALSE, message = FALSE}
ap = function (myfile) paste0(file.path(readSlide("ds_normalizing_data", file = FALSE), "/figure", myfile))
```

There are many ways to define the distance between two points, e.g., $Z_i = (X_i, Y_i)$ and $Z_j = (X_j, Y_j)$:

```{r,echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.5\\textwidth"}

Colors = colorspace::rainbow_hcl(3)
cbbPalette = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}

par(mar = c(4,4,0,0))
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19,
  xlab = "Variable X (Dimension 1)", ylab = "Variable Y (Dimension 2)")
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(Z[i]), expression(Z[j])), adj = c(1.5, 0))
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = Colors[1])
legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(Colors[1],1))

text(x = 5, y = 1, expression(d(Z[i],Z[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = Colors[1])

asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(Z[i],Z[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
```
\vspace{-10pt}

- \small manhattan: sum up the absolute distances in each dimension.

- \small euclidean: remember Pythagoras theorem from school?


## Normalizing Data: Distances

It is often a good idea to *normalize* the data before computing distances, especially when the scale of variables is different, e.g. the euclidean distance between the point $Z_1$ and $Z_2$:

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=9}
par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0))

dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(Z[1],Z[2])~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))

dat$height = dat$height/100
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(Z[1],Z[2])~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))

```

On the right plot, the distance is dominated by `shoe size`.

## Normalizing Data: Distances

\small
The normalized variable $\tilde{X}_{\texttt{shoe.size}}$ is computed by <!-- Normalization of the \texttt{shoe.size} variable means: -->
\[\tilde{X}_{\texttt{shoe.size}} = \tfrac{X_{\texttt{shoe.size}}-\bar{X}_{\texttt{shoe.size}}}{s_{X_{\texttt{shoe.size}}}}.\]
Distances based on normalized data are better comparable and **robust** in terms of linear transformations (e.g., conversion of physical units).

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.6\\textwidth"}
par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0))

#dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
dat = as.data.frame(scale(dat))
plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height",
  pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(Z[1],Z[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))
```

<!-- ## Normalizing: Covariance vs. Correlation -->

<!-- The **covariance** $\sigma_{XY} = Cov(X,Y)$ of two variables $X$ and $Y$ can be estimated by the sample covariance -->
<!-- $$s_{XY}=\dfrac{1}{n-1}\sum_{i=1}^{n}{(x_i-\bar{x})(y_i-\bar{y})}.$$ -->
<!-- It can be interpreted as the \emph{average of the rectangle} with width $x_i - \bar{x}$ and height $y_i - \bar{y}$. -->

<!-- ```{r, echo = FALSE, fig.height = 3, fig.width = 4, out.width="100%"} -->
<!-- ind = 1 -->

<!-- data = as.data.frame(matrix(c(0.4,-0.8,-0.5,0.7,0.7,0.8,-0.6,-0.9), ncol = 2)) -->
<!-- data$label = paste0("Point ", 1:4) -->
<!-- p = ggplot(data) + geom_point(aes(x = V1, y = V2)) + -->
<!--   geom_hline(yintercept = mean((data$V2)), linetype = "dashed") +  -->
<!--   geom_vline(xintercept = mean((data$V1)), linetype = "dashed")  + xlim(c(-1, 1)) + ylim(c(-1, 1)) + -->
<!--   geom_text(aes(x=mean((data$V1)), label="bar(x)", y=1), parse = T, hjust = 0) + -->
<!--   geom_text(aes(x=-1, label="bar(y)", y=mean(data$V2)), parse = T, vjust = 0) + -->
<!--   theme(axis.title = element_text(size = 14), -->
<!--     plot.title = element_text(size =  rel(4)))  -->

<!-- inds = 1:4 -->
<!-- for(ind in inds) { -->
<!--   col = ifelse(ind%%2 == 0, "blue", "red") -->
<!--   p = p + annotate("rect",xmin = mean((data$V1)), ymin = mean((data$V2)), -->
<!--     xmax = (data$V1)[ind], ymax = (data$V2)[ind], alpha = .1, -->
<!--     color = col, fill = col) + -->
<!--     annotate("text", x = mean(c((data$V1)[ind], mean((data$V1)))),  -->
<!--       y = (data$V2)[ind],  -->
<!--       vjust = ifelse((data$V2)[ind]>0, -0.5, 1),  -->
<!--       label = paste0("(x[", ind, "]-bar(x))"), parse = T, colour = col) + -->
<!--     annotate("text", x = (data$V1)[ind],  -->
<!--       vjust = ifelse((data$V1)[ind]>0, 1, -0.5),  -->
<!--       y = mean(c((data$V2)[ind], mean((data$V2)))),  -->
<!--       label = paste0("(y[", ind, "]-bar(y))"), -->
<!--       parse = T, colour = col, angle = 90) + -->
<!--     geom_point(x = (data$V1)[ind], y = (data$V2)[ind],  -->
<!--       color = col, alpha = .1, size = 4) -->
<!-- } -->

<!-- p + geom_text_repel(aes(x = V1, y = V2, label = label)) + xlab("") + ylab("") -->

<!-- ``` -->

<!-- ## Normalizing: Covariance vs. Correlation -->

<!-- The covariance is based on computing the area of the rectangles, which requires  computing differences: -->

<!-- ```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=9} -->
<!-- par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0)) -->

<!-- dat = data.frame(shoe.size = c(46, 40, 43), height = c(180, 172, 179)) -->
<!-- dat = rbind(dat, colMeans(dat)) -->
<!-- plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)", -->
<!--   pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.01), ylim = range(dat$height)*c(0.99, 1.01)) -->
<!-- #lines(x = dat$shoe.size[-3], y = dat$height[-3]) -->
<!-- text(x = dat$shoe.size, y = dat$height,  -->
<!--   c(expression("("~X[2]~","~Y[2]~")"), -->
<!--     expression("("~X[1]~","~Y[1]~")"),  -->
<!--     expression("("~X[3]~","~Y[3]~")"),  -->
<!--     expression("("~bar(X)~","~bar(Y)~")")), adj = c(1.1, 0)) -->
<!-- lines(x = c(dat$shoe.size[2], dat$shoe.size[2], dat$shoe.size[4], dat$shoe.size[4]), y = c(dat$height[2], dat$height[2], dat$height[2], dat$height[4])) -->

<!-- text(x = 40, y = 172, expression("(40-43) (172-177) = 15"), adj = c(0,1.1)) -->

<!-- dat$height = dat$height/100 -->
<!-- plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)", -->
<!--   pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.01), ylim = range(dat$height)*c(0.99, 1.01)) -->
<!-- #lines(x = dat$shoe.size[-3], y = dat$height[-3]) -->

<!-- text(x = dat$shoe.size, y = dat$height, c(expression("("~X[2]~","~Y[2]~")"), -->
<!--     expression("("~X[1]~","~Y[1]~")"),  -->
<!--     expression("("~X[3]~","~Y[3]~")"),  -->
<!--     expression("("~bar(X)~","~bar(Y)~")")), adj = c(1.1, 0)) -->
<!-- lines(x = c(dat$shoe.size[2], dat$shoe.size[2], dat$shoe.size[4], dat$shoe.size[4]), y = c(dat$height[2], dat$height[2], dat$height[2], dat$height[4])) -->

<!-- text(x = 40, y = 1.72, expression("(40-43) (1.72-1.77) = 0.15"), adj = c(0,1.1)) -->
<!-- ``` -->

<!-- The correlation is just the scaled covariance and won't change if, e.g., the variables are measured in different physical units. -->

## Normalizing: Covariance vs. Correlation

The **variance** of a normalized variable is always 1, its mean is always 0.

The **covariance** of two normalized variables $\tilde{X} = \tfrac{X - \bar{X}}{s_X}$ and $\tilde{Y} = \tfrac{Y - \bar{Y}}{s_Y}$ is the same as the **correlation** of the non-normalized variables $X$ and $Y$. 

One can proof this with the help of
$$s_{\tilde{X}\tilde{Y}}=\tfrac{1}{n-1}\sum_{i=1}^{n}{(\tilde{x}_i-\bar{\tilde{x}})(\tilde{y}_i-\bar{\tilde{y}})} = \hdots =  \tfrac{1}{n-1}\sum_{i=1}^{n}{\tfrac{(x_i-\bar{x})}{s_{X}}\tfrac{(y_i-\bar{y})}{s_{Y}}} = r_{XY}.$$

<!-- ## Distances between Observations -->

<!-- ```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.65\\textwidth"} -->
<!-- par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0)) -->

<!-- dat = data.frame(shoe.size = c(46, 40, 43), height = c(180, 172, 179)) -->
<!-- dat = as.data.frame(scale(dat)) -->
<!-- dat = rbind(dat, colMeans(dat)) -->
<!-- plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)", -->
<!--   pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02)) -->
<!-- #lines(x = dat$shoe.size[-3], y = dat$height[-3]) -->

<!-- text(x = dat$shoe.size, y = dat$height, c(expression("("~X[2]~","~Y[2]~")"), -->
<!--     expression("("~X[1]~","~Y[1]~")"),  -->
<!--     expression("("~X[3]~","~Y[3]~")"),  -->
<!--     expression("("~bar(X)~","~bar(Y)~")")), adj = c(1.01, 0)) -->
<!-- lines(x = c(dat$shoe.size[2], dat$shoe.size[2], dat$shoe.size[4], dat$shoe.size[4]), y = c(dat$height[2], dat$height[2], dat$height[2], dat$height[4])) -->

<!-- text(x = 40, y = 1.72, expression("(40-43) (1.72-1.77) = 0.15"), adj = c(0,1)) -->

<!-- ``` -->

<!-- ## Summary -->

<!--   - Categorical variables can be summarized by tables and visualized by bar plots. -->
<!--   - For metric variables, -->
<!--     - we can compute summary statistics such as the mean (location) and the standard deviation (spread). -->
<!--     - we can use histograms or estimated density plots to visualize their distribution. -->
<!--     - we can fit known density functions by estimating the density parameters (e.g., $\mu$ and $\sigma$ in case of a normal distribution). -->
<!--   - The scatter plot is a powerful visualization tool for multivariate data. -->
<!--   - The shape of a multivariate normal distribution is controlled by its covariance matrix. -->

```{r, include=FALSE, echo=FALSE, message=FALSE}
library("knitr")
set.seed(1)
options(scipen = 1, digits = 4, width=70)
Colors = colorspace::rainbow_hcl(3)
cbbPalette = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```









## PCA Intuition

```{r, include = FALSE, echo = FALSE, message = FALSE} 
ap = function (myfile) paste0(file.path(readSlide("ds_pca_1", file = FALSE), "/figure", myfile))
library(MASS)
library(GGally)
library(factoextra)
library(ggrepel)
library(rafalib)
library(colorspace)
```

*Motivational example I*:

- Variable $x_1$ explains most of the variation.
- Variable $x_2$ has a lower variance than $x_1$.
- If we disregard $x_2$ and project the points into the 1-dimensional space of $x_1$, we do not lose much information w.r.t. variability. 

\begin{center}
  \includegraphics[width = \textwidth]{`r ap("pca1.png")`}
\end{center}

## PCA Intuition

*Motivational example II*:

- $x_1$ and $x_2$ are correlated and have similar variances.
- Find a new orthogonal axes (e.g. PC1 and PC2), where PC1 explains most of the variation.
- Rotate the points and consider PC1 and PC2 as new coordinate system (situation as in the previous example).
- We can now project points onto PC1 and disregard PC2 (hopefully without losing much information).

\vspace{-20pt}
\begin{center}
  \includegraphics[width = \textwidth]{`r ap("pca2.png")`}
\end{center}
\vspace{-20pt}

<!-- [SEE THIS ANIMATION.](https://i.stack.imgur.com/Q7HIP.gif) -->

## PCA Intuition

*General procedure*:

1. Rotate the original $p$-dimensional coordinate system until the first PC that explains most of the variation is found.
1. Fix the first PC and proceed with rotating the remaining $p-1$ coordinates until the second PC (which is orthogonal to the first PC) is found that explains most of the *remaining* variation, etc.
1. We can reduce the dimensions by projecting the points onto the first, say $k<p$, PC.

## PCA Intuition: Find first PC

```{r, results="asis", echo = FALSE, fig.height=7, fig.width=7, out.width="70%"}
Colors = colorspace::rainbow_hcl(3)
# code from https://github.com/genomicsclass/labs/blob/master/highdim/PCA.Rmd 
mypar()
n = 50
col = Colors[1]
set.seed(123)

Y=t(mvrnorm(n,c(0,0), matrix(c(1,0.9,0.9,1),2,2)))

thelim = c(-3,3)
#par(xaxs='i',yaxs='i',mar=c(1,1,3,3)+0.1)
plot(Y[1,], Y[2,], xlim=thelim, ylim=thelim, axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label

for (rotate in c(0,-0.5,-1, -3, -300, 3, 1)) {
  cat("  \n## ", "PCA Intuition: Animation  \n \\addtocounter{framenumber}{-1}  \n")
  u = matrix(c(1,rotate),ncol=1)
  u = u/sqrt(sum(u^2))
  w=t(u)%*%Y
  mypar(1,1)
  plot(t(Y), main=paste("Variance of projected points:",round(tcrossprod(w)/(n-1),2) ), 
    xlim=thelim, ylim=thelim, axes=F, xlab = "", ylab = "")
  #box()
  arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label
  #abline(h=0,lty=2)
  #abline(v=0,lty=2)
  abline(0,rotate,col=col)
  abline(0,ifelse(rotate == 0, 300, -1/rotate),col=col)
  Z = u%*%w
  for(i in seq(along=w))
    segments(Z[1,i],Z[2,i],Y[1,i],Y[2,i],lty=2)
  points(t(Z), col=col, pch=16, cex=0.8)  
  legend("topleft", lty = c(1,2, NA), pch = c(NA,NA,16), legend = c("rotated coordinate system", "original coordinate system", "projected points"), col = c(col, "black", col))
}
text(3, 3, labels = "PC1", col = col, pos = 1)
text(3, -3, labels = "PC2", col = col, pos = 3)
```

## PCA Intuition: Reduce dimensionality

Rotate the points and use PC1 and PC2 as new coordinate system. 

Here, the PC1 axis explains most of the variance:

\vspace{-10px}

```{r, echo = FALSE, fig.height=7, fig.width=7, out.width="60%"}
a = pi/4
Yrotated = t(Y) %*% matrix(c(cos(a), -sin(a), sin(a), cos(a)), ncol = 2, byrow = T)
mypar(1,1)
plot(Yrotated, 
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
```

## PCA Intuition: Reduce dimensionality

Dimensionality can be reduced by projecting the points onto the PC1 (and by disregarding PC2).
The hope is that we won't lose much information this way.

\vspace{-20px}

```{r, echo = FALSE, fig.height=7, fig.width=7, out.width="60%"}
mypar(1,1)
plot(Yrotated, 
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
#abline(h=0,lty=2, col=col)
#abline(v=0,lty=2, col=col)
u = matrix(c(1,0),ncol=1)
u = u/sqrt(sum(u^2))
wrotated = t(u)%*%t(Yrotated)
Zrotated = u%*%wrotated
for(i in seq(along=w))
  segments(Zrotated[1,i], Zrotated[2,i], t(Yrotated)[1,i], t(Yrotated)[2,i], lty=2)
points(t(Zrotated), col=col, pch=16, cex=0.8) 

# plot(t(Zrotated), col=col, pch=16, cex=0.8, xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
# arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
# mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
# mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
```

<!-- ## PCA Intuition: Animation -->

<!-- Rotate back to see what kind of information we losed: -->

<!-- ```{r, echo = FALSE} -->
<!-- rotate = 1 -->
<!-- u = matrix(c(1,rotate),ncol=1) -->
<!-- u = u/sqrt(sum(u^2)) -->
<!-- w=t(u)%*%Y -->
<!-- mypar(1,1) -->
<!-- plot(NA, main="",xlim=thelim,ylim=thelim, xlab=expression(x[1]), ylab=expression(x[2])) -->
<!-- abline(0,rotate,col=col) -->
<!-- abline(0,ifelse(rotate == 0, 300, -1/rotate),col=col) -->
<!-- Z = u%*%w -->
<!-- points(t(Z), col=col, pch=16, cex=0.8) -->
<!-- ``` -->

## PCA Intuition: Summary

**Idea:** Transform an original set of correlated metric variables to a new set of uncorrelated (orthogonal) metric variables, called principal components (PC), that explain the variability in the data.

- The objective is to investigate if only a few PC account for most of the variability in the original data.

- If the objective is fulfilled, we can use fewer PCs to reduce the dimensionality. 
  
- The PCs remove collinearity of the input variables as they are orthogonal to each other.

## PCA Intuition: Final Remarks

- PCA is used for dimensionality reduction by disregaring dimensions with lower variability.
- There is always an information loss, especially for other criteria. 
- E.g., dimensionality reduciton can worsen the classification accuracy when the task is to classify two groups:

\begin{center}
  \includegraphics[width = \textwidth]{`r ap("pca3.png")`}
\end{center}

<!-- ## Deriving the First PC Mathematically -->

<!-- The first PC of the observations is the linear combination -->
<!-- $$\mathbf{pc}_1 = a_{11} \mathbf{x}_1 + a_{12}\mathbf{x}_2 + \hdots + a_{1p}\mathbf{x}_p.$$ -->

<!-- - The **loading vectors** $\mathbf{a}_1, \hdots, \mathbf{a}_p$ contain the coefficient weights of the linear combination of each PC, e.g. the loading vector of the $k$-th PC is $\mathbf{a}_k = (a_{k1}, \hdots, a_{kp})^\top$. -->

<!-- - For identifiability reasons, a restriction must be placed on the loading vectors, i.e. the sum of squares should be one ($\mathbf{a}_k^\top \mathbf{a}_k = 1$). -->

## Deriving the First PC Mathematically

<!-- PCA works on the covariance matrix $\Sigma$ or correlation matrix. -->

<!-- - We need an estimate of the covariance matrix $\Sigma$ of data matrix $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$ that contains only metric variables. -->

Aim: Find a new set of variables (PC scores) $\mathbf{pc}_1, \ldots, \mathbf{pc}_p$ based on the original data $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$ so that

- each PC score $\mathbf{pc}_1, \hdots, \mathbf{pc}_p$ is a linear combination of the original metric variables with coefficient weights (so-called **loading vectors**) $\mathbf{a}_1, \hdots, \mathbf{a}_p$, i.e.
  \[
  \mathbf{pc}_j = a_{j1}\mathbf{x}_1 + a_{j2}\mathbf{x}_2 + \ldots + a_{jp}\mathbf{x}_p = \mathbf{X} \mathbf{a}_j.
  \]

- the set is mutually uncorrelated: $Cov(\mathbf{pc}_j,\mathbf{pc}_k) = 0, \; \forall j \neq k.$

- the variances of the PC scores decrease:
  \[\lambda_1 > \lambda_2 > \ldots > \lambda_p, \;\;\;  \text{where } \lambda_k := Var(\mathbf{pc}_k). \]


## Deriving the First PC Mathematically

<!-- PCA works on the covariance matrix $\Sigma$ of the data matrix $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$. -->
We look for the loading vector $\mathbf{a}_1 = (a_{11}, a_{21}, \hdots, a_{p1})^\top$ that maximizes the variance of $\mathbf{pc}_1$:
  \[
  \max_{\mathbf{a}_1} \ Var(\mathbf{pc}_1) = Var(\mathbf{X} \mathbf{a}_1) = \mathbf{a}_1^\top \Sigma \mathbf{a}_1
  \]
subject to the normalization constraint $\mathbf{a}_1^\top \mathbf{a}_1 = \sum_{k=1}^p a_{k1}^2 = 1$.

The constraint is required for identifiability reasons, otherwise we could maximize the variance by just increasing the values in $\mathbf{a}_1$.

Repeat this maximization step for the other PCs and additionally use the orthogonality constraint, i.e. for the second PC: $$\mathbf{a}_2^\top\mathbf{a}_1 = 0.$$









## Example: The Olympic Heptathlon Data

```{r, include = FALSE, echo = FALSE, message = FALSE} 
ap = function (myfile) paste0(file.path(readSlide("ds_pca_R", file = FALSE), "/figure", myfile))
library(MASS)
library(GGally)
library(factoextra)
library(ggrepel)
library(rafalib)
library(colorspace)
```

The `heptathlon` data set in the R package `HSAUR3` contains the competition results of 25 athletes in 7 disciplines for the Olympics held in Seoul in 1988.

```{r, message=FALSE, out.width="0.5\\textwidth", eval = 2}
install.packages("HSAUR3")
data(heptathlon, package = "HSAUR3")
```

- **Aim**: Rank the athletes according to their overall performance in all 7 disciplines.

- **Idea**: Use PCA to reduce the dimensionality (i.e., reduce the results of the 7 disciplines to one dimension) and compare the scores of the first PC with the official scores.

## Example: The Olympic Heptathlon Data

Variables of the `heptathlon` data:

- `hurdles`: results 100m hurdles (in seconds).

- `highjump`: results high jump (in m).

- `shot`: results shot putt (in m).

- `run200m`: results 200m race (in seconds).

- `longjump`: results long jump (in m).

- `javelin`: results javelin (in m).

- `run800m`: results 800m race (in seconds).

- `score`: total score of the official scoring system.

## Example: The Olympic Heptathlon Data

The variables `hurdles`, `run200m` and `run800m` are time measurements, i.e. low values are better. 
For all other variables high values are better. 

Results of the best and worst participant:

```{r, size='tiny', R.options=list(width = 200), echo = FALSE}
kable(heptathlon[c(1,25),])
```

We use negative time measurements so that higher values are better and therefore all variables have the same direction:

```{r, size='tiny', echo = FALSE}
heptathlon$hurdles = -heptathlon$hurdles #with(heptathlon, max(hurdles)-hurdles)
heptathlon$run200m = -heptathlon$run200m #with(heptathlon, max(run200m)-run200m)
heptathlon$run800m = -heptathlon$run800m #with(heptathlon, max(run800m)-run800m)
```

```{r, size='tiny', R.options=list(width = 200), echo = FALSE}
kable(heptathlon[c(1,25),])
```

## Scatter Plot Matrix

```{r, fig.align="center", out.width = "0.95\\textwidth", echo = FALSE, message=FALSE}
#plot(heptathlon[, -8], pch = 19)
ggpairs(heptathlon[, -8]) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Correlation Matrix

We can compute all pairwise correlations of the variables (without the `score` column):

```{r, R.options=list(digits = 2)}
(cor.mat = cor(heptathlon[, -8]))
```

<!-- After removing the outlier: -->

<!-- ```{r, R.options=list(digits = 2), size = "tiny"} -->
<!-- heptathlon = heptathlon[!worst, ] -->
<!-- (without.outlier = cor(heptathlon[,-score])) -->
<!-- ``` -->

## Correlogram 

```{r, message=FALSE, fig.align="center", fig.height = 5, out.width = "0.75\\textwidth", echo = -c(1, 3)}
#par(mfrow = c(1, 2)) #  tl.srt = 60
library(corrplot)
body(corrplot)[[57]] = substitute(oldpar <- par(mar = mar))
corrplot(cor.mat, type = "upper", addCoef.col = "black")
```

## Perform PCA using `princomp()`

Remember: The first PC is the linear combination
  \[
  \mathbf{pc}_1 = a_{11} \mathbf{x}_1 + a_{12}\mathbf{x}_2+ \dots + a_{1p}\mathbf{x}_p
  \]
and has the largest sample variance among all other PCs.

<!-- - In R, PCA can be done using the functions `princomp()` and `prcomp()` (both contained in the R package `stats`). -->

- In R, the `princomp()` function carries out a PCA via an eigendecomposition of the sample covariance matrix $\Sigma$.

- If variables are on very different scales, PCA should be carried out on the correlation matrix (which is equivalent to the correlation matrix if normalized variables are used).


## Perform PCA using `princomp()`

- As the variables of the heptathlon data are on different scales, we perform the PCA based on the correlation matrix.

    ```{r, echo=1, results="hide"}
    hept.pca = princomp(heptathlon[, -8], cor = TRUE)
    ```

- Alternatively, we could also perform the PCA based on the covariance matrix but on the normalized heptathlon data.

- The resulting object of `princomp()` contains at least:

    - The loadings $\mathbf{a}_{1}, \hdots, \mathbf{a}_{p}$,
    - The PC scores $\mathbf{pc}_{1}, \hdots, \mathbf{pc}_{p}$ and
    - The variance $\lambda_1, \hdots, \lambda_p$ (or standard deviation) of the PC scores.

## Loadings 

The loadings $\mathbf{a}_{1}, \hdots, \mathbf{a}_{p}$ are contained in

```{r, eval = FALSE}
hept.pca$loadings
```

```{r, echo = FALSE}
kable(unclass(round(hept.pca$loadings, 2)))
```

## Loadings 

Visualize the coefficient weights (loadings) of the linear combinations of the PC scores:
```{r, message=FALSE, fig.align="center", fig.height = 5, out.width = "0.6\\textwidth"}
corrplot(hept.pca$loadings, is.corr = FALSE)
```

## Loadings

- Check the normalization constraint of the first PC:
    ```{r, size = "tiny"}
    (a1 = hept.pca$loadings[, 1]) # loadings of the first PC
    a1 %*% a1 # check constraint: is sum of squares equal to 1?
    ```
  
- Check the orthogonality constraint of the first two PCs:
    ```{r, size = "tiny"}
    (a2 = hept.pca$loadings[, 2]) # loadings of the second PC
    a1 %*% a2 # check if 1st and 2nd PC are orthogonal
    ```


## Loadings 

If we perform PCA on the covariance matrix (without normalizing the data), each component mainly loads on a single variable:

```{r, fig.align="center", fig.height = 5, out.width = "0.5\\textwidth"}
hept.pca.cov = princomp(heptathlon[, -8], cor = FALSE)
corrplot(hept.pca.cov$loadings, is.corr = FALSE)
```

Reason: Variables have very different scales (e.g., time measurement of 200m and 800m run).

## Proportion of Explained Variance

- The total variance of the $p$ PC scores is equal the total variance of the original variables, i.e.,
  $$\textstyle\sum_{j=1}^p \lambda_j = s_1^2 +s_2^2 + \dots + s_p^2,$$
  where $\lambda_j$ is the variance of the $j$th PC and $s_j^2$ is the sample variance of variable $\mathbf{x}_j$.

- The proportion of explained variance of the $j$-th PC is $$\tfrac{\lambda_j}{\sum_{j=1}^p \lambda_j}.$$ 

- The first $k$ PCs account for a proportion $$\tfrac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j}.$$

## Proportion of Explained Variance

In R, the proportion of explained variance can be obtained by

```{r, R.options=list(width = 55)}
summary(hept.pca)
```

**Question**: How do we choose the number of PCs?

## Choosing the Number of PCs

Two simple rules of thumb for choosing the number of PCs:

1. Retain the first $k$ components, which explain a large proportion of the total variation, e.g., 70-80\%.
   
2. Use a scree plot: Plot the component variances vs. the component number and look for an \textit{elbow}.
For components after the \textit{elbow}, the variance decreases more slowly.

<!-- 3. If the correlation matrix was used for the PCA: Retain only the components with variances greater than one. -->

```{r, fig.align="center", fig.height = 4, out.width = "0.7\\textwidth", echo = FALSE}
fviz_eig(hept.pca, xlab = "principal component")
```

## PC Scores vs. Official Scores

The first PC explains $63,72\%$ of the variation, the loadings of the first PC are:

```{r}
hept.pca$loadings[,1]
```

Dimensionality reduction:

- Project all 8 features onto the first PC.
- Compare the scores of the first PC with the official scores used to rank the athletes.

## PC Scores vs. Official Scores

The scores of the first PC $\mathbf{pc}_1$ have a similar ranking as the scores of the official scoring system, i.e., we can reduce the dimension to the first PC without losing much information:

```{r, fig.align="center", fig.width = 8, fig.height = 6, out.width= "0.6\\textwidth", echo = FALSE}
d = data.frame(official.score = heptathlon$score, pca.score = hept.pca$scores[row.names(heptathlon),1], name = row.names(heptathlon))
ggplot(d, aes(official.score, pca.score, label = name)) +
    geom_text_repel() +
    geom_point(color = 'red') + ylab("1st PC scores") + xlab("Official Scores")
# par(mar = c(4,4,1,1))
# #cor(heptathlon$score, hept.pca$scores[,1])
# plot(heptathlon$score, hept.pca$scores[,1], xlab = "Official Scores", ylab = "1st PC scores")
# text(heptathlon$score, hept.pca$scores[,1], row.names(heptathlon), pos = 4, cex = 0.5)
```


