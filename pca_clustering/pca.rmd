






## PCA Intuition

```{r, include = FALSE, echo = FALSE, message = FALSE} 
ap = function (myfile) paste0(file.path(readSlide("ds_pca_1", file = FALSE), "/figure", myfile))
library(MASS)
library(GGally)
library(factoextra)
library(ggrepel)
library(rafalib)
library(colorspace)
```

*Motivational example I*:

- Variable $x_1$ explains most of the variation.
- Variable $x_2$ has a lower variance than $x_1$.
- If we disregard $x_2$ and project the points into the 1-dimensional space of $x_1$, we do not lose much information w.r.t. variability. 

\begin{center}
  \includegraphics[width = \textwidth]{`r ap("pca1.png")`}
\end{center}

## PCA Intuition

*Motivational example II*:

- $x_1$ and $x_2$ are correlated and have similar variances.
- Find a new orthogonal axes (e.g. PC1 and PC2), where PC1 explains most of the variation.
- Rotate the points and consider PC1 and PC2 as new coordinate system (situation as in the previous example).
- We can now project points onto PC1 and disregard PC2 (hopefully without losing much information).

\vspace{-20pt}
\begin{center}
  \includegraphics[width = \textwidth]{`r ap("pca2.png")`}
\end{center}
\vspace{-20pt}

<!-- [SEE THIS ANIMATION.](https://i.stack.imgur.com/Q7HIP.gif) -->

## PCA Intuition

*General procedure*:

1. Rotate the original $p$-dimensional coordinate system until the first PC that explains most of the variation is found.
1. Fix the first PC and proceed with rotating the remaining $p-1$ coordinates until the second PC (which is orthogonal to the first PC) is found that explains most of the *remaining* variation, etc.
1. We can reduce the dimensions by projecting the points onto the first, say $k<p$, PC.

## PCA Intuition: Find first PC

```{r, results="asis", echo = FALSE, fig.height=7, fig.width=7, out.width="70%"}
Colors = colorspace::rainbow_hcl(3)
# code from https://github.com/genomicsclass/labs/blob/master/highdim/PCA.Rmd 
mypar()
n = 50
col = Colors[1]
set.seed(123)

Y=t(mvrnorm(n,c(0,0), matrix(c(1,0.9,0.9,1),2,2)))

thelim = c(-3,3)
#par(xaxs='i',yaxs='i',mar=c(1,1,3,3)+0.1)
plot(Y[1,], Y[2,], xlim=thelim, ylim=thelim, axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label

for (rotate in c(0,-0.5,-1, -3, -300, 3, 1)) {
  cat("  \n## ", "PCA Intuition: Animation  \n \\addtocounter{framenumber}{-1}  \n")
  u = matrix(c(1,rotate),ncol=1)
  u = u/sqrt(sum(u^2))
  w=t(u)%*%Y
  mypar(1,1)
  plot(t(Y), main=paste("Variance of projected points:",round(tcrossprod(w)/(n-1),2) ), 
    xlim=thelim, ylim=thelim, axes=F, xlab = "", ylab = "")
  #box()
  arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label
  #abline(h=0,lty=2)
  #abline(v=0,lty=2)
  abline(0,rotate,col=col)
  abline(0,ifelse(rotate == 0, 300, -1/rotate),col=col)
  Z = u%*%w
  for(i in seq(along=w))
    segments(Z[1,i],Z[2,i],Y[1,i],Y[2,i],lty=2)
  points(t(Z), col=col, pch=16, cex=0.8)  
  legend("topleft", lty = c(1,2, NA), pch = c(NA,NA,16), legend = c("rotated coordinate system", "original coordinate system", "projected points"), col = c(col, "black", col))
}
text(3, 3, labels = "PC1", col = col, pos = 1)
text(3, -3, labels = "PC2", col = col, pos = 3)
```

## PCA Intuition: Reduce dimensionality

Rotate the points and use PC1 and PC2 as new coordinate system. 

Here, the PC1 axis explains most of the variance:

\vspace{-10px}

```{r, echo = FALSE, fig.height=7, fig.width=7, out.width="60%"}
a = pi/4
Yrotated = t(Y) %*% matrix(c(cos(a), -sin(a), sin(a), cos(a)), ncol = 2, byrow = T)
mypar(1,1)
plot(Yrotated, 
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
```

## PCA Intuition: Reduce dimensionality

Dimensionality can be reduced by projecting the points onto the PC1 (and by disregarding PC2).
The hope is that we won't lose much information this way.

\vspace{-20px}

```{r, echo = FALSE, fig.height=7, fig.width=7, out.width="60%"}
mypar(1,1)
plot(Yrotated, 
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
#abline(h=0,lty=2, col=col)
#abline(v=0,lty=2, col=col)
u = matrix(c(1,0),ncol=1)
u = u/sqrt(sum(u^2))
wrotated = t(u)%*%t(Yrotated)
Zrotated = u%*%wrotated
for(i in seq(along=w))
  segments(Zrotated[1,i], Zrotated[2,i], t(Yrotated)[1,i], t(Yrotated)[2,i], lty=2)
points(t(Zrotated), col=col, pch=16, cex=0.8) 

# plot(t(Zrotated), col=col, pch=16, cex=0.8, xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
# arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
# mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
# mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
```

## PCA Intuition: Summary

**Idea:** Transform an original set of correlated metric variables to a new set of uncorrelated (orthogonal) metric variables, called principal components (PC), that explain the variability in the data.

- The objective is to investigate if only a few PC account for most of the variability in the original data.

- If the objective is fulfilled, we can use fewer PCs to reduce the dimensionality. 
  
- The PCs remove collinearity of the input variables as they are orthogonal to each other.

## PCA Intuition: Final Remarks

- PCA is used for dimensionality reduction by disregaring dimensions with lower variability.
- There is always an information loss, especially for other criteria. 
- E.g., dimensionality reduciton can worsen the classification accuracy when the task is to classify two groups:

\begin{center}
  \includegraphics[width = \textwidth]{`r ap("pca3.png")`}
\end{center}


## Deriving the First PC Mathematically

Aim: Find a new set of variables (PC scores) $\mathbf{pc}_1, \ldots, \mathbf{pc}_p$ based on the original data $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$ so that

- each PC score $\mathbf{pc}_1, \hdots, \mathbf{pc}_p$ is a linear combination of the original metric variables with coefficient weights (so-called **loading vectors**) $\mathbf{a}_1, \hdots, \mathbf{a}_p$, i.e.
  \[
  \mathbf{pc}_j = a_{j1}\mathbf{x}_1 + a_{j2}\mathbf{x}_2 + \ldots + a_{jp}\mathbf{x}_p = \mathbf{X} \mathbf{a}_j.
  \]

- the set is mutually uncorrelated: $Cov(\mathbf{pc}_j,\mathbf{pc}_k) = 0, \; \forall j \neq k.$

- the variances of the PC scores decrease:
  \[\lambda_1 > \lambda_2 > \ldots > \lambda_p, \;\;\;  \text{where } \lambda_k := Var(\mathbf{pc}_k). \]


## Deriving the First PC Mathematically

<!-- PCA works on the covariance matrix $\Sigma$ of the data matrix $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$. -->
We look for the loading vector $\mathbf{a}_1 = (a_{11}, a_{21}, \hdots, a_{p1})^\top$ that maximizes the variance of $\mathbf{pc}_1$:
  \[
  \max_{\mathbf{a}_1} \ Var(\mathbf{pc}_1) = Var(\mathbf{X} \mathbf{a}_1) = \mathbf{a}_1^\top \Sigma \mathbf{a}_1
  \]
subject to the normalization constraint $\mathbf{a}_1^\top \mathbf{a}_1 = \sum_{k=1}^p a_{k1}^2 = 1$.

The constraint is required for identifiability reasons, otherwise we could maximize the variance by just increasing the values in $\mathbf{a}_1$.

Repeat this maximization step for the other PCs and additionally use the orthogonality constraint, i.e. for the second PC: $$\mathbf{a}_2^\top\mathbf{a}_1 = 0.$$

## Example: The Olympic Heptathlon Data

```{r, include = FALSE, echo = FALSE, message = FALSE} 
ap = function (myfile) paste0(file.path(readSlide("ds_pca_R", file = FALSE), "/figure", myfile))
library(MASS)
library(GGally)
library(factoextra)
library(ggrepel)
library(rafalib)
library(colorspace)
```

The `heptathlon` data set in the R package `HSAUR3` contains the competition results of 25 athletes in 7 disciplines for the Olympics held in Seoul in 1988.

```{r, message=FALSE, out.width="0.5\\textwidth", eval = 2}
install.packages("HSAUR3")
data(heptathlon, package = "HSAUR3")
```

- **Aim**: Rank the athletes according to their overall performance in all 7 disciplines.

- **Idea**: Use PCA to reduce the dimensionality (i.e., reduce the results of the 7 disciplines to one dimension) and compare the scores of the first PC with the official scores.

## Example: The Olympic Heptathlon Data

Variables of the `heptathlon` data:

- `hurdles`: results 100m hurdles (in seconds).

- `highjump`: results high jump (in m).

- `shot`: results shot putt (in m).

- `run200m`: results 200m race (in seconds).

- `longjump`: results long jump (in m).

- `javelin`: results javelin (in m).

- `run800m`: results 800m race (in seconds).

- `score`: total score of the official scoring system.

## Example: The Olympic Heptathlon Data

The variables `hurdles`, `run200m` and `run800m` are time measurements, i.e. low values are better. 
For all other variables high values are better. 

Results of the best and worst participant:

```{r, size='tiny', R.options=list(width = 200), echo = FALSE}
kable(heptathlon[c(1,25),])
```

We use negative time measurements so that higher values are better and therefore all variables have the same direction:

```{r, size='tiny', echo = FALSE}
heptathlon$hurdles = -heptathlon$hurdles #with(heptathlon, max(hurdles)-hurdles)
heptathlon$run200m = -heptathlon$run200m #with(heptathlon, max(run200m)-run200m)
heptathlon$run800m = -heptathlon$run800m #with(heptathlon, max(run800m)-run800m)
```

```{r, size='tiny', R.options=list(width = 200), echo = FALSE}
kable(heptathlon[c(1,25),])
```

## Scatter Plot Matrix

```{r, fig.align="center", out.width = "0.95\\textwidth", echo = FALSE, message=FALSE}
#plot(heptathlon[, -8], pch = 19)
ggpairs(heptathlon[, -8]) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Correlation Matrix

We can compute all pairwise correlations of the variables (without the `score` column):

```{r, R.options=list(digits = 2)}
(cor.mat = cor(heptathlon[, -8]))
```

## Correlogram 

```{r, message=FALSE, fig.align="center", fig.height = 5, out.width = "0.75\\textwidth", echo = -c(1, 3)}
#par(mfrow = c(1, 2)) #  tl.srt = 60
library(corrplot)
body(corrplot)[[57]] = substitute(oldpar <- par(mar = mar))
corrplot(cor.mat, type = "upper", addCoef.col = "black")
```

## Perform PCA using `princomp()`

Remember: The first PC is the linear combination
  \[
  \mathbf{pc}_1 = a_{11} \mathbf{x}_1 + a_{12}\mathbf{x}_2+ \dots + a_{1p}\mathbf{x}_p
  \]
and has the largest sample variance among all other PCs.

<!-- - In R, PCA can be done using the functions `princomp()` and `prcomp()` (both contained in the R package `stats`). -->

- In R, the `princomp()` function carries out a PCA via an eigendecomposition of the sample covariance matrix $\Sigma$.

- If variables are on very different scales, PCA should be carried out on the correlation matrix (which is equivalent to the correlation matrix if normalized variables are used).


## Perform PCA using `princomp()`

- As the variables of the heptathlon data are on different scales, we perform the PCA based on the correlation matrix.

    ```{r, echo=1, results="hide"}
    hept.pca = princomp(heptathlon[, -8], cor = TRUE)
    ```

- Alternatively, we could also perform the PCA based on the covariance matrix but on the normalized heptathlon data.

- The resulting object of `princomp()` contains at least:

    - The loadings $\mathbf{a}_{1}, \hdots, \mathbf{a}_{p}$,
    - The PC scores $\mathbf{pc}_{1}, \hdots, \mathbf{pc}_{p}$ and
    - The variance $\lambda_1, \hdots, \lambda_p$ (or standard deviation) of the PC scores.

## Loadings 

The loadings $\mathbf{a}_{1}, \hdots, \mathbf{a}_{p}$ are contained in

```{r, eval = FALSE}
hept.pca$loadings
```

```{r, echo = FALSE}
kable(unclass(round(hept.pca$loadings, 2)))
```

## Loadings 

Visualize the coefficient weights (loadings) of the linear combinations of the PC scores:
```{r, message=FALSE, fig.align="center", fig.height = 5, out.width = "0.6\\textwidth"}
corrplot(hept.pca$loadings, is.corr = FALSE)
```

## Loadings

- Check the normalization constraint of the first PC:
    ```{r, size = "tiny"}
    (a1 = hept.pca$loadings[, 1]) # loadings of the first PC
    a1 %*% a1 # check constraint: is sum of squares equal to 1?
    ```
  
- Check the orthogonality constraint of the first two PCs:
    ```{r, size = "tiny"}
    (a2 = hept.pca$loadings[, 2]) # loadings of the second PC
    a1 %*% a2 # check if 1st and 2nd PC are orthogonal
    ```


## Loadings 

If we perform PCA on the covariance matrix (without normalizing the data), each component mainly loads on a single variable:

```{r, fig.align="center", fig.height = 5, out.width = "0.5\\textwidth"}
hept.pca.cov = princomp(heptathlon[, -8], cor = FALSE)
corrplot(hept.pca.cov$loadings, is.corr = FALSE)
```

Reason: Variables have very different scales (e.g., time measurement of 200m and 800m run).

## Proportion of Explained Variance

- The total variance of the $p$ PC scores is equal the total variance of the original variables, i.e.,
  $$\textstyle\sum_{j=1}^p \lambda_j = s_1^2 +s_2^2 + \dots + s_p^2,$$
  where $\lambda_j$ is the variance of the $j$th PC and $s_j^2$ is the sample variance of variable $\mathbf{x}_j$.

- The proportion of explained variance of the $j$-th PC is $$\tfrac{\lambda_j}{\sum_{j=1}^p \lambda_j}.$$ 

- The first $k$ PCs account for a proportion $$\tfrac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j}.$$

## Proportion of Explained Variance

In R, the proportion of explained variance can be obtained by

```{r, R.options=list(width = 55)}
summary(hept.pca)
```

**Question**: How do we choose the number of PCs?

## Choosing the Number of PCs

Two simple rules of thumb for choosing the number of PCs:

1. Retain the first $k$ components, which explain a large proportion of the total variation, e.g., 70-80\%.
   
2. Use a scree plot: Plot the component variances vs. the component number and look for an \textit{elbow}.
For components after the \textit{elbow}, the variance decreases more slowly.

<!-- 3. If the correlation matrix was used for the PCA: Retain only the components with variances greater than one. -->

```{r, fig.align="center", fig.height = 4, out.width = "0.7\\textwidth", echo = FALSE}
fviz_eig(hept.pca, xlab = "principal component")
```

## PC Scores vs. Official Scores

The first PC explains $63,72\%$ of the variation, the loadings of the first PC are:

```{r}
hept.pca$loadings[,1]
```

Dimensionality reduction:

- Project all 8 features onto the first PC.
- Compare the scores of the first PC with the official scores used to rank the athletes.

## PC Scores vs. Official Scores

The scores of the first PC $\mathbf{pc}_1$ have a similar ranking as the scores of the official scoring system, i.e., we can reduce the dimension to the first PC without losing much information:

```{r, fig.align="center", fig.width = 8, fig.height = 6, out.width= "0.6\\textwidth", echo = FALSE}
d = data.frame(official.score = heptathlon$score, pca.score = hept.pca$scores[row.names(heptathlon),1], name = row.names(heptathlon))
ggplot(d, aes(official.score, pca.score, label = name)) +
    geom_text_repel() +
    geom_point(color = 'red') + ylab("1st PC scores") + xlab("Official Scores")
# par(mar = c(4,4,1,1))
# #cor(heptathlon$score, hept.pca$scores[,1])
# plot(heptathlon$score, hept.pca$scores[,1], xlab = "Official Scores", ylab = "1st PC scores")
# text(heptathlon$score, hept.pca$scores[,1], row.names(heptathlon), pos = 4, cex = 0.5)
```


