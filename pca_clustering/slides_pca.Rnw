% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(xtable)
library(MASS)
library(GGally)
library(factoextra)
library(ggrepel)
library(rafalib)
library(colorspace)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{PCA}
\lecture{Introduction to Machine Learning}
\sloppy

\section{Introduction}

\begin{frame}{Suggested Literature}
  \begin{itemize}
  \item Hastie, T., Tibshirani, R., Friedman, J. (2009): The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer. 
  \item James, G., Witten, D., Hastie, T., Tibshirani, R. (2013): An Introduction to Statistical Learning with Applications in R. Springer.
  \item Aggarwal, C. C., \& Reddy, C. K. (Eds.). (2013). Data Clustering: Algorithms and Applications. CRC press.
  \end{itemize}
\end{frame}


\begin{frame}{Unsupervised Learning}
\begin{itemize}
  \item Supervised machine learning deals with *labeled* data, i.e., we have input data $x$ and the outcome $y$ of past events.
  \item Here, the aim is to learn relationships between $x$ and $y$.
  \item Unsupervised machine learning deals with data that is *unlabeled*, i.e., there is no real output $y$.
  \item Here, the aim is to search for patterns within the inputs $x$.
\end{itemize}

\end{frame}

\begin{frame}{Clustering Task}
\textbf{Goal:} Group data into similar clusters (or estimate fuzzy membership
  probabilities)

<<echo=FALSE, fig.height=4>>=
  df4 = getTaskData(iris.task)
  m = as.matrix(cbind(df4$Petal.Length, df4$Petal.Width), ncol = 2)
  cl = (kmeans(m,3))
  df4$cluster = factor(cl$cluster)
  centers = as.data.frame(cl$centers)
  ggplot(data = df4, aes(x = Petal.Length, y = Petal.Width, color = cluster )) +
   geom_point(size = 4) +
   geom_point(data = centers, aes(x = V1, y = V2, color = 'Center')) +
   geom_point(data = centers, aes(x = V1,y = V2, color = 'Center'), size = 90, alpha = .3) +
   theme(legend.position = "none")
@

\end{frame}

\begin{frame}{Clustering: Customer Segmentation}

\begin{itemize}
  \item In marketing, customer segmentation is an important task to understand customer needs and to meet with customer expectations.
  \item Customer data is partitioned in terms of similiarities and the characteristics of each group are summarized.
  \item Marketing strategies are designed and prioritized according to the group size.
\end{itemize}

\lz
Example Use Cases:

\begin{itemize}
  \item Personalized ads (e.g., recommend articles).
  \item Music/Movie recommendation systems.
\end{itemize}

\end{frame}


\begin{frame}{Clustering: Image Compression}
\begin{itemize}
  \item An image consists of pixels arranged in rows and columns.
  \item Each pixel contains \textbf{RGB} color information, i.e., a mix of the intensity of 3 \textbf{primary colors}: \textbf{R}ed, \textbf{G}reen and \textbf{B}lue.
  \item Each primary color takes intensity values between 0 and 255.
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/rgb-cube.png}

\tiny Source: By Ferlixwangg \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0}, from \href{https://commons.wikimedia.org/wiki/File:Rgb-cube.gif}{Wikimedia Commons}.
\end{center}
\end{frame}

\begin{frame}{Clustering: Image Compression}
An image can be compressed by reducing its color information, i.e., by replacing similar colors of each pixel with, say, $k$ distinct colors.

\textbf{Example}: 

<<echo = FALSE, fig.height=5, fig.width=10>>=
library(jpeg)

img = readJPEG("figure_man/colorful_bird.jpg") # Read the image
imgDm = dim(img)
# We re-arranged the 3D array into a dataset that has the coordinates of the pixel and the color information (R, G and B)
imgRGB = data.frame(
  x = rep(1:imgDm[2], each = imgDm[1]),
  y = rep(imgDm[1]:1, imgDm[2]),
  R = as.vector(img[,,1]),
  G = as.vector(img[,,2]),
  B = as.vector(img[,,3])
)

par(mfrow = c(1,2), mar = c(0,0,2,0))
# We now plot the pixels (pch = 15) with its RGB color info
plot(imgRGB$x, imgRGB$y, col = rgb(imgRGB[c("R", "G", "B")]), pch = 15, axes = F, main = "Original Image", xlab = "", ylab = "")
box()

k = 16
kMeans = kmeans(imgRGB[, c("R", "G", "B")], centers = k)
predRGB = kMeans$centers[kMeans$cluster,]

plot(imgRGB$x, imgRGB$y, col = rgb(predRGB), pch = 15, axes = F, main = "Image using 16 Colors", xlab = "", ylab = "")
box()
@

\end{frame}

\begin{frame}{Dimensionality Reduction Task}

\textbf{Goal}: Describe data with fewer features (reduce number of columns). \
$\Rightarrow$ there will always be an information loss.

\begin{center}
\begin{tabular}{ | c | c | c | c | c | c |}
    \hline
      & & & & & \\ \hline
      & & & & & \\ \hline
      & & & & & \\
    \hline
  \end{tabular} $\Rightarrow$
    \begin{tabular}{ | c | c | c |}
    \hline
      & &  \\ \hline
      & &  \\ \hline
      & &  \\
    \hline
  \end{tabular}
\end{center}

Unsupervised Methods:
\begin{itemize}
  \item Principle Component Analysis (PCA).
  \item Factor Analysis (FA).
  \item Feature filter methods.
\end{itemize} 
\lz

Supervised Methods:
\begin{itemize}
  \item Linear Discriminant Analysis (LDA).
  \item Feature filter methods.
\end{itemize}      
\end{frame}


\section{Principal Component Analysis}


\begin{frame}{Normalizing Data}

A variable $X$ can be normalized by substracting its values with the mean $\bar{X}$ and dividing by the standard deviation $s_X$, e.g. $\tilde{X} = \tfrac{X - \bar{X}}{s_X}.$

\lz

\textbf{Example:}
<<echo = FALSE, results='hide'>>=
bh = t(data.frame(body.height = c("Person A" = 180, "Person B" = 172, "Person C" = 175)))
@

Consider the following body heights measured in different units:

<<echo=FALSE, results='asis'>>=
bh.m = bh/100
bh.feet = round(bh/30.48, 4)

options(xtable.comment = FALSE)
tab = rbind(cbind(bh, "mean" = mean(bh), "sd" = sd(bh)), 
  cbind(bh.m, "mean" = mean(bh.m), "sd" = sd(bh.m)),
  cbind(bh.feet, "mean" = mean(bh.feet), "sd" = sd(bh.feet)))
row.names(tab) = c("body height (cm)", "body height (m)", "body height (feet)")

xtable(tab, align = c("c|", "c", "c", "c", "|c", "c"))
@

After normalizing, we always obtain the normalized body height (no matter which unit was used):

<<echo = FALSE, results='asis'>>=
bh.norm = (bh - mean(bh))/sd(bh)
bh.norm = cbind(bh.norm, "mean" = round(mean(bh.norm), 0), "sd" = sd(bh.norm))
row.names(bh.norm) = "normalized body height"
xtable(bh.norm, align = c("c|", "c", "c", "c", "|c", "c"))
@
\end{frame}

\begin{frame}{Normalizing Data}

Normalizing all variables in a data set, can have several advantages:

\begin{itemize}
  \item It puts all variables into *comparable* units, i.e., we make sure that all normalized variables have mean 0 and standard deviation of 1.
  \item It can avoid numerical instabilites in several algorithms, e.g. if a variable has very low / high values.
  \item It helps in computing meaningful *distances* between observations.
\end{itemize}

\end{frame}

\begin{frame}{Normalizing Data:Distances}

There are many ways to define the distance between two points, e.g., $Z_i = (X_i, Y_i)$ and $Z_j = (X_j, Y_j)$:

<<echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.5\\textwidth">>=

Colors = colorspace::rainbow_hcl(3)
cbbPalette = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}

par(mar = c(4,4,0,0))
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19,
  xlab = "Variable X (Dimension 1)", ylab = "Variable Y (Dimension 2)")
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(Z[i]), expression(Z[j])), adj = c(1.5, 0))
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = Colors[1])
legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(Colors[1],1))

text(x = 5, y = 1, expression(d(Z[i],Z[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = Colors[1])

asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(Z[i],Z[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
@

\vspace{-10pt}

- \small manhattan: sum up the absolute distances in each dimension.

- \small euclidean: remember Pythagoras theorem from school?


\end{frame}

\begin{frame}{Normalizing Data:Distances}

It is often a good idea to \textit{normalize} the data before computing distances, especially when the scale of variables is different, e.g. the euclidean distance between the point $Z_1$ and $Z_2$:

<<echo=FALSE, fig.align="center", fig.height=4, fig.width=9>>=
par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0))

dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(Z[1],Z[2])~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))

dat$height = dat$height/100
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(Z[1],Z[2])~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))
@

On the right plot, the distance is dominated by \enquote{shoe size}.

\end{frame}

\begin{frame}{Normalizing Data: Distances}

\small
The normalized variable $\tilde{X}_{\texttt{shoe.size}}$ is computed by <!-- Normalization of the \texttt{shoe.size} variable means: -->
\[\tilde{X}_{\texttt{shoe.size}} = \tfrac{X_{\texttt{shoe.size}}-\bar{X}_{\texttt{shoe.size}}}{s_{X_{\texttt{shoe.size}}}}.\]
Distances based on normalized data are better comparable and **robust** in terms of linear transformations (e.g., conversion of physical units).

<<echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.6\\textwidth">>=
par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0))

#dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
dat = as.data.frame(scale(dat))
plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height",
  pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(Z[1],Z[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))
@

\end{frame}


\begin{frame}{Normalizing: Covariance vs. Correlation}

The \textbf{variance} of a normalized variable is always 1, its mean is always 0.

The \textbf{covariance} of two normalized variables $\tilde{X} = \tfrac{X - \bar{X}}{s_X}$ and $\tilde{Y} = \tfrac{Y - \bar{Y}}{s_Y}$ is the same as the \textbf{correlation} of the non-normalized variables $X$ and $Y$. 

One can proof this with the help of
$$s_{\tilde{X}\tilde{Y}}=\tfrac{1}{n-1}\sum_{i=1}^{n}{(\tilde{x}_i-\bar{\tilde{x}})(\tilde{y}_i-\bar{\tilde{y}})} = \hdots =  \tfrac{1}{n-1}\sum_{i=1}^{n}{\tfrac{(x_i-\bar{x})}{s_{X}}\tfrac{(y_i-\bar{y})}{s_{Y}}} = r_{XY}.$$

<<include=FALSE, echo=FALSE, message=FALSE>>=
library("knitr")
set.seed(1)
options(scipen = 1, digits = 4, width=70)
Colors = colorspace::rainbow_hcl(3)
cbbPalette = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
@

\end{frame}

\begin{frame}{PCA Intuition}

\textit{Motivational example I}:

\begin{itemize}
  \item Variable $x_1$ explains most of the variation.
  \item Variable $x_2$ has a lower variance than $x_1$.
  \item If we disregard $x_2$ and project the points into the 1-dimensional space of $x_1$, we do not lose much information w.r.t. variability. 
\end{itemize}

\begin{center}
  \includegraphics[width = \textwidth]{figure_man/pca1.png}
\end{center}

\end{frame}

\begin{frame}{PCA Intuition}

\textit{Motivational example II}:

\begin{itemize}
\item $x_1$ and $x_2$ are correlated and have similar variances.
\item Find a new orthogonal axes (e.g. PC1 and PC2), where PC1 explains most of the variation.
\item Rotate the points and consider PC1 and PC2 as new coordinate system (situation as in the previous example).
\item We can now project points onto PC1 and disregard PC2 (hopefully without losing much information).
\end{itemize}

\vspace{-20pt}
\begin{center}
  \includegraphics[width = \textwidth]{figure_man/pca2.png}
\end{center}
\vspace{-20pt}


%% <!-- [SEE THIS ANIMATION.](https://i.stack.imgur.com/Q7HIP.gif) -->

\end{frame}

\begin{frame}{PCA Intuition}

\textit{General procedure}:

\begin{enumerate}
  \item Rotate the original $p$-dimensional coordinate system until the first PC that explains most of the variation is found.
  \item Fix the first PC and proceed with rotating the remaining $p-1$ coordinates until the second PC (which is orthogonal to the first PC) is found that explains most of the *remaining* variation, etc.
  \item We can reduce the dimensions by projecting the points onto the first, say $k<p$, PC.
\end{enumerate}

\end{frame}

\begin{frame}{PCA Intuition: Find first PC}

PROBLEM HERE! MULTIPLE SLIDES DOES NOT RENDER! 
<<echo=FALSE, fig.height=7, fig.width=7>>=

Colors = colorspace::rainbow_hcl(3)
# code from https://github.com/genomicsclass/labs/blob/master/highdim/PCA.Rmd 
mypar()
n = 50
col = Colors[1]
set.seed(123)

Y=t(mvrnorm(n,c(0,0), matrix(c(1,0.9,0.9,1),2,2)))

thelim = c(-3,3)
#par(xaxs='i',yaxs='i',mar=c(1,1,3,3)+0.1)
plot(Y[1,], Y[2,], xlim=thelim, ylim=thelim, axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label

for (rotate in c(0,-0.5,-1, -3, -300, 3, 1)) {
  # cat("  \n## ", "PCA Intuition: Animation  \n \\addtocounter{framenumber}{-1}  \n")
  u = matrix(c(1,rotate),ncol=1)
  u = u/sqrt(sum(u^2))
  w=t(u)%*%Y
  mypar(1,1)
  plot(t(Y), main=paste("Variance of projected points:",round(tcrossprod(w)/(n-1),2) ), 
    xlim=thelim, ylim=thelim, axes=F, xlab = "", ylab = "")
  #box()
  arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
  mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
  mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label
  #abline(h=0,lty=2)
  #abline(v=0,lty=2)
  abline(0,rotate,col=col)
  abline(0,ifelse(rotate == 0, 300, -1/rotate),col=col)
  Z = u%*%w
  for(i in seq(along=w))
    segments(Z[1,i],Z[2,i],Y[1,i],Y[2,i],lty=2)
  points(t(Z), col=col, pch=16, cex=0.8)  
  legend("topleft", lty = c(1,2, NA), pch = c(NA,NA,16), legend = c("rotated coordinate system", "original coordinate system", "projected points"), col = c(col, "black", col))
}
text(3, 3, labels = "PC1", col = col, pos = 1)
text(3, -3, labels = "PC2", col = col, pos = 3)
@

\end{frame}

\begin{frame}{PCA Intuition: Reduce dimensionality}

Rotate the points and use PC1 and PC2 as new coordinate system. 
\lz
Here, the PC1 axis explains most of the variance:

\vspace{-10px}

<<echo = FALSE, fig.height=7, fig.width=7, out.width="60%">>=
a = pi/4
Yrotated = t(Y) %*% matrix(c(cos(a), -sin(a), sin(a), cos(a)), ncol = 2, byrow = T)
mypar(1,1)
plot(Yrotated, 
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
@

\end{frame}

\begin{frame}{PCA Intuition: Reduce dimensionality}

Dimensionality can be reduced by projecting the points onto the PC1 (and by disregarding PC2).
The hope is that we won't lose much information this way.

\vspace{-20px}

<<echo = FALSE, fig.height=7, fig.width=7, out.width="60%">>=
mypar(1,1)
plot(Yrotated, 
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
#abline(h=0,lty=2, col=col)
#abline(v=0,lty=2, col=col)
u = matrix(c(1,0),ncol=1)
u = u/sqrt(sum(u^2))
wrotated = t(u)%*%t(Yrotated)
Zrotated = u%*%wrotated
for(i in seq(along=w))
  segments(Zrotated[1,i], Zrotated[2,i], t(Yrotated)[1,i], t(Yrotated)[2,i], lty=2)
points(t(Zrotated), col=col, pch=16, cex=0.8) 

# plot(t(Zrotated), col=col, pch=16, cex=0.8, xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
# arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
# mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
# mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
@

\end{frame}

\begin{frame}{PCA Intuition: Summary}

\textbf{Idea:} Transform an original set of correlated metric variables to a new set of uncorrelated (orthogonal) metric variables, called principal components (PC), that explain the variability in the data.

  \begin{itemize}
    \item The objective is to investigate if only a few PC account for most of the variability in the original data.
    \item If the objective is fulfilled, we can use fewer PCs to reduce the dimensionality. 
    \item The PCs remove collinearity of the input variables as they are orthogonal to each other.
  \end{itemize}

\end{frame}

\begin{frame}{PCA Intuition: Final Remarks}

  \begin{itemize}
    \item PCA is used for dimensionality reduction by disregaring dimensions with lower variability.
    \item There is always an information loss, especially for other criteria. 
    \item E.g., dimensionality reduciton can worsen the classification accuracy when the task is to classify two groups:
  \end{itemize}
\begin{center}
  \includegraphics[width = \textwidth]{figure_man/pca3.png}
\end{center}

\end{frame}

\begin{frame}{Deriving the First PC Mathematically}

Aim: Find a new set of variables (PC scores) $\mathbf{pc}_1, \ldots, \mathbf{pc}_p$ based on the original data $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$ so that

\begin{itemize}
  \item each PC score $\mathbf{pc}_1, \hdots, \mathbf{pc}_p$ is a linear combination of the original metric variables with coefficient weights (so-called **loading vectors**) $\mathbf{a}_1, \hdots, \mathbf{a}_p$, i.e.
    \[
    \mathbf{pc}_j = a_{j1}\mathbf{x}_1 + a_{j2}\mathbf{x}_2 + \ldots + a_{jp}\mathbf{x}_p = \mathbf{X} \mathbf{a}_j.
    \]

  \item the set is mutually uncorrelated: $Cov(\mathbf{pc}_j,\mathbf{pc}_k) = 0, \; \forall j \neq k.$

  \item the variances of the PC scores decrease:
    \[\lambda_1 > \lambda_2 > \ldots > \lambda_p, \;\;\;  \text{where } \lambda_k := Var(\mathbf{pc}_k). \]
\end{itemize}

\end{frame}

\begin{frame}{Deriving the First PC Mathematically}

%% <!-- PCA works on the covariance matrix $\Sigma$ of the data matrix $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$. -->
We look for the loading vector $\mathbf{a}_1 = (a_{11}, a_{21}, \hdots, a_{p1})^\top$ that maximizes the variance of $\mathbf{pc}_1$:
  \[
  \max_{\mathbf{a}_1} \ Var(\mathbf{pc}_1) = Var(\mathbf{X} \mathbf{a}_1) = \mathbf{a}_1^\top \Sigma \mathbf{a}_1
  \]
subject to the normalization constraint $\mathbf{a}_1^\top \mathbf{a}_1 = \sum_{k=1}^p a_{k1}^2 = 1$.

\lz 
The constraint is required for identifiability reasons, otherwise we could maximize the variance by just increasing the values in $\mathbf{a}_1$.

\lz
Repeat this maximization step for the other PCs and additionally use the orthogonality constraint, i.e. for the second PC: $$\mathbf{a}_2^\top\mathbf{a}_1 = 0.$$

\end{frame}




\endlecture
