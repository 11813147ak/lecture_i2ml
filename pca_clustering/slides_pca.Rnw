% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{PCA}
\lecture{Introduction to Machine Learning}
\sloppy

\section{Introduction}

\begin{frame}{Suggested Literature}
  \begin{itemize}
  \item Hastie, T., Tibshirani, R., Friedman, J. (2009): The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer. 
  \item James, G., Witten, D., Hastie, T., Tibshirani, R. (2013): An Introduction to Statistical Learning with Applications in R. Springer.
  \item Aggarwal, C. C., \& Reddy, C. K. (Eds.). (2013). Data Clustering: Algorithms and Applications. CRC press.
  \end{itemize}
\end{frame}


\begin{frame}{Unsupervised Learning}
\begin{itemize}
  \item Supervised machine learning deals with *labeled* data, i.e., we have input data $x$ and the outcome $y$ of past events.
  \item Here, the aim is to learn relationships between $x$ and $y$.
  \item Unsupervised machine learning deals with data that is *unlabeled*, i.e., there is no real output $y$.
  \item Here, the aim is to search for patterns within the inputs $x$.
\end{itemize}

\end{frame}

\begin{frame}{Clustering Task}
\textbf{Goal:} Group data into similar clusters (or estimate fuzzy membership
  probabilities)

<<echo=FALSE, fig.height=4>>=
  df4 = getTaskData(iris.task)
  m = as.matrix(cbind(df4$Petal.Length, df4$Petal.Width), ncol = 2)
  cl = (kmeans(m,3))
  df4$cluster = factor(cl$cluster)
  centers = as.data.frame(cl$centers)
  ggplot(data = df4, aes(x = Petal.Length, y = Petal.Width, color = cluster )) +
   geom_point(size = 4) +
   geom_point(data = centers, aes(x = V1, y = V2, color = 'Center')) +
   geom_point(data = centers, aes(x = V1,y = V2, color = 'Center'), size = 90, alpha = .3) +
   theme(legend.position = "none")
@

\end{frame}

\begin{frame}{Clustering: Customer Segmentation}

\begin{itemize}
  \item In marketing, customer segmentation is an important task to understand customer needs and to meet with customer expectations.
  \item Customer data is partitioned in terms of similiarities and the characteristics of each group are summarized.
  \item Marketing strategies are designed and prioritized according to the group size.
\end{itemize}

\lz
Example Use Cases:

\begin{itemize}
  \item Personalized ads (e.g., recommend articles).
  \item Music/Movie recommendation systems.
\end{itemize}

\end{frame}


\begin{frame}{Clustering: Image Compression}
\begin{itemize}
  \item An image consists of pixels arranged in rows and columns.
  \item Each pixel contains \textbf{RGB} color information, i.e., a mix of the intensity of 3 \textbf{primary colors}: \textbf{R}ed, \textbf{G}reen and \textbf{B}lue.
  \item Each primary color takes intensity values between 0 and 255.
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/rgb-cube.png}

\tiny Source: By Ferlixwangg \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0}, from \href{https://commons.wikimedia.org/wiki/File:Rgb-cube.gif}{Wikimedia Commons}.
\end{center}
\end{frame}

\begin{frame}{Clustering: Image Compression}
An image can be compressed by reducing its color information, i.e., by replacing similar colors of each pixel with, say, $k$ distinct colors.

\textbf{Example}: 

<<echo = FALSE, fig.height=5, fig.width=10>>=
library(jpeg)

img = readJPEG("figure_man/colorful_bird.jpg") # Read the image
imgDm = dim(img)
# We re-arranged the 3D array into a dataset that has the coordinates of the pixel and the color information (R, G and B)
imgRGB = data.frame(
  x = rep(1:imgDm[2], each = imgDm[1]),
  y = rep(imgDm[1]:1, imgDm[2]),
  R = as.vector(img[,,1]),
  G = as.vector(img[,,2]),
  B = as.vector(img[,,3])
)

par(mfrow = c(1,2), mar = c(0,0,2,0))
# We now plot the pixels (pch = 15) with its RGB color info
plot(imgRGB$x, imgRGB$y, col = rgb(imgRGB[c("R", "G", "B")]), pch = 15, axes = F, main = "Original Image", xlab = "", ylab = "")
box()

k = 16
kMeans = kmeans(imgRGB[, c("R", "G", "B")], centers = k)
predRGB = kMeans$centers[kMeans$cluster,]

plot(imgRGB$x, imgRGB$y, col = rgb(predRGB), pch = 15, axes = F, main = "Image using 16 Colors", xlab = "", ylab = "")
box()
@

\end{frame}

\begin{frame}{Dimensionality Reduction Task}

\textbf{Goal}: Describe data with fewer features (reduce number of columns). \
$\Rightarrow$ there will always be an information loss.

\begin{center}
\begin{tabular}{ | c | c | c | c | c | c |}
    \hline
      & & & & & \\ \hline
      & & & & & \\ \hline
      & & & & & \\
    \hline
  \end{tabular} $\Rightarrow$
    \begin{tabular}{ | c | c | c |}
    \hline
      & &  \\ \hline
      & &  \\ \hline
      & &  \\
    \hline
  \end{tabular}
\end{center}

Unsupervised Methods:
\begin{itemize}
  \item Principle Component Analysis (PCA).
  \item Factor Analysis (FA).
  \item Feature filter methods.
\end{itemize} 
\lz

Supervised Methods:
\begin{itemize}
  \item Linear Discriminant Analysis (LDA).
  \item Feature filter methods.
\end{itemize}      
\end{frame}

\endlecture
