# Partitioning Clustering Methods

## Optimal Partitioning Clustering

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2, 
                     function(x) 
                       rgb(x[1], x[2], x[3], alpha=alpha))  
}
```

**Hierarchical clustering:** 
Stepwise merging (agglomerative methods) or dividing (divisive methods) of clusters based on distances and linkages. 
The number of clusters are selected by splitting the dendrogram at a specific threshold for the ``height'' after visual inspection.

**Partitioning clustering:** 
Partitions the $N$ observations into a predefined number of $K$ clusters by optimizing a numerical criterion. The most common partitioning methods are:

- $K$-means
- $K$-medians
- $K$-medoids (Partitioning Around Medoids (PAM))

## $K$-means

$K$-means partitions the $N$ observations into $K$ predefined clusters $C_1, C_2, \hdots, C_K$ by minimizing the **compactness**, i.e. the **within-cluster variation** of all clusters using
$$\textstyle\sum_{k=1}^K \sum_{i \in C_k} \|X_i-\bar{X}_k\|_2^2 \rightarrow \min,$$ 
where $\bar{X}_k = \frac{1}{N_k} \sum_{i \in C_k} X_i$ is the centroid of cluster $k$ and $N_k$ is the number of observations in cluster $k$. 

<!--Hence, equivalently we seek a clustering $C$ that minimizes the 
within-cluster variation.-->

```{r, echo = FALSE, fig.height=3.5, fig.width=6.5, out.width="0.6\\textwidth", message=FALSE}
Colors = colorspace::rainbow_hcl(3)
library(MASS)
library(cluster)
library(pdist)
set.seed(123456)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,0), Sigma = diag(2)*0.2)
cl2 = mvrnorm(n = n, mu = c(3,-1.5), Sigma = diag(2)*0.3)
cl3 = mvrnorm(n = n, mu = c(2,0.25), Sigma = diag(2)*0.6)

e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
e3 = ellipsoidhull(cl3)
dat = rbind(cl1, cl2, cl3)
d1 = as.matrix(pdist(cl1, cl2))
d2 = as.matrix(pdist(cl1, cl3))
d3 = as.matrix(pdist(cl2, cl3))

par(mar = c(0,0,0,0))

pr = (rbind(predict(e1), predict(e2), predict(e3)))
plot(dat, pch = 19, col = rep(Colors, each = n),
  xlab = "", ylab = "", axes = F, xlim = range(pr[,1]), ylim = range(pr[,2]))
lines(predict(e1), col = Colors[1])
lines(predict(e2), col = Colors[2])
lines(predict(e3), col = Colors[3])


centers = as.data.frame(rbind(
  c("x" = e2$loc[1], "y" = e2$loc[2]), 
  c("x" = e1$loc[1], "y" = e1$loc[2])))
#lines(centers, col = "black", lwd = 2)
arrows(x0 = centers$x[1], x1 = centers$x[2], 
  y0 = centers$y[1], y1 = centers$y[2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(centers)
text(xy[1], xy[2], label = "between cluster variation", col = "black")

text(e1$loc[1], e1$loc[2], label = expression(C[1]), col = Colors[1], pos = 1)
text(e2$loc[1], e2$loc[2], label = expression(C[2]), col = Colors[2], pos = 1)
text(e3$loc[1], e3$loc[2], label = expression(C[3]), col = Colors[3], pos = 3)

d = predict(e1)
#summary(d)
xy = dat[c(14,16),]
#lines(xy, col = "black", lwd = 2)
arrows(x0 = xy[1,1], x1 = xy[2,1], 
  y0 = xy[1,2], y1 = xy[2,2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(xy)
text(xy[1], xy[2], label = "within-cluster variation", col = "black")

```


## $K$-means

Idea: Consider every possible partition of $N$ observations into $K$ clusters and select the one with the lowest **within-cluster variation**.

Problem: Requires trying all possible assignments of $N$ observations into $K$ clusters, which in practice is nearly impossible (Hothorn et al., 2009, p.322):

|$N$     |$K$     | Number of possible partitions|
|-------:|:------:|:-----------------------------|
|15      | 3      | 2.375.101                    |
|20      | 4      | 45.232.115.901               |
|100     | 5      | $10^{68}$                    |

\tiny{Hothorn, T., Everitt, B. S. (2009). A handbook of statistical analyses using R. Chapman and Hall/CRC.}

## $K$-means

Use an approximation:

1. **Initialization:** Choose $K$ arbitrary observations to be the initial cluster 
   centers.

2. **Assignment:** Assign every observation to the cluster with the closest center.

3. **Update:** Compute the new center of each cluster as the mean of its members.

4. Repeat (2) and (3) until the centers do not move.
 

## $K$-means Example

```{r, fig.width = 7, fig.height = 5, fig.align="center", echo=FALSE, message=FALSE, results="asis"}
set.seed(1234)
pts = iris[, 3:4]

iters = 3
K = 3
km = vector("list", iters)
centers = vector("list", iters + 1)

centers[[1]] = pts[sample.int(nrow(pts), K), , drop = FALSE] #matrix(c(1,0.5,2,1,7,2), ncol = 2, byrow = TRUE)

par(mar = c(4,4,3,0))
plot(pts, pch = 19, col = "gray70")
points(centers[[1]][, 1], centers[[1]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
title(main = "Iteration 0", line = 2)
title(main = "Choose K arbitrary observations to be the initial cluster centers", line = 1, cex.main = 0.8)
  
for (i in 1:iters) {
  cat("  \n##", "$K$-means Example  \n  \\addtocounter{framenumber}{-1}  \n")
  km[[i]] = kmeans(pts, centers = centers[[i]], nstart = 1, algorithm = "Lloyd", iter.max = 1)
  
  col = km[[i]]$cluster
  plot(pts, pch = 19, col = col)
  title(main = paste("Iteration", i), line = 2)
  title(main = "Assign observations to nearest cluster center", line = 1, cex.main = 0.8)
  points(centers[[i]][, 1], centers[[i]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
  
  cat("  \n")
  cat("  \n##", "$K$-means Example  \n  \\addtocounter{framenumber}{-1}  \n")
  
  centers[[i + 1]] = km[[i]]$centers
  plot(pts, pch = 19, col = col)
  points(centers[[i + 1]][, 1], centers[[i + 1]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
  title(main = paste("Iteration", i), line = 2)
  title(main = "Compute new cluster center", line = 1, cex.main = 0.8)
  cat("  \n")
}
```

<!-- Select 3 random points and assume them to be cluster centers for the clusters to be created. -->

## $K$-means in R

The $K$-means algorithm is part of the base distribution in R, given by
the `kmeans` function (using `algorithm = "Lloyd"`):

```{r, echo = TRUE, size = "tiny"}
km = kmeans(iris[,3:4], centers = 3, nstart = 100, iter.max = 100, algorithm = "Lloyd")
str(km)
```

## $K$-means in R

The final cluster assignments can be visualized by

```{r, fig.height = 5, out.width = "\\textheight", fig.align="center", echo = 2}
par(mar = c(4,4,1,1))
plot(iris[,3:4], pch = 19, col = km$cluster)
```





## Properties of $K$-means

- $K$-means is based on computing the mean, which is sensitive to outliers and can only be computed for numerical data.

- The **within-cluster variation** is reduced in each iteration. 
  In R, the maximum number of iterations is specified by `iter.max`.

- The final result is typically not the best result that globally minimizes the **within-cluster variation**. \
  $\rightarrow$ would only be possible after trying all possible partitions!

- $K$-means can be restarted multiple times using `nstart`.
  The clustering with the smallest within-cluster variation is then selected as 
  the best solution.
  
## Properties of $K$-means

- $K$-means produces different clusters depending on the initial centers and always converges, e.g.:
<!-- In fact, it takes $\leq K^n$ iterations (why?) -->

```{r, echo=FALSE, fig.width=8, fig.height=4, fig.align="center", out.width="\\textwidth"}
set.seed(0)
x = rbind(
  scale(matrix(rnorm(2*20, sd = 0.4), ncol=2), center=c(1,1), scale = F),
  scale(matrix(rnorm(2*30, sd = 0.4), ncol=2), center=-c(1,1), scale = F)
  )
x = rbind(x, matrix(rep(seq(-1,1, length.out = 10), each = 2), ncol = 2, byrow = TRUE))
#x = rbind(x, matrix(runif(2*10, min(x), max(x)), ncol=2))
d = dist(x)

init.col = add.alpha("#000000", alpha = 0.5)
par(mfrow=c(1,3), mar = c(3,3,1,1))
for(i in 1:3) {
  rand = c(1, 25, 54)+i
  km1 = kmeans(x, centers = x[rand, ], nstart = 1, algorithm = "Lloyd")
  plot(x, pch = 19, col = km1$cluster, ylim = c(-1.5, 2))
  points(x[rand, 1], x[rand, 2], pch = 4, cex = 2, lwd = 2, col = init.col)
  if(i == 1) legend("topleft", pch = 4, pt.lwd = 2, pt.cex = 2, legend = "Initial Cluster Centers",
    col = init.col)
}

```

## Choice of $K$

- Many methods exist for choosing the number of clusters $K$ (there is no perfect solution).

- The easiest method is to apply $K$-means for different $K$ and plot the **within-cluster variation** for each number of $K$.
  
- The **within-cluster variation** always decreases with increasing number of clusters. 

- An **"elbow"** in the plot might indicate a useful solution.

```{r, echo=FALSE, fig.height=4.5, fig.align="center", out.width="0.7\\textheight"}
set.seed(1)
K = 8
wss = numeric(K)
for (k in 1:K) {
  km = kmeans(iris[,3:4], centers = k, nstart = 10, iter.max = 100, algorithm = "Lloyd")
  wss[k] = km$tot.withinss
}
par(mar = c(4,4,1,1))
plot(1:K, wss, type = "b", xlab = "K (number of clusters)", ylab = "within-cluster variation")
```



## $K$-medoids 

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2, 
                     function(x) 
                       rgb(x[1], x[2], x[3], alpha=alpha))  
}
```

$K$-medoids clustering 

- is strongly related to $K$-means and is realized by the **Partitioning Around Medoids (PAM)** algorithm.
- uses cluster medoids as representative clusters, i.e. **real data points** instead of **artificial data points** (such as the cluster centers as in $K$-means) are used. 
- is less sensitive to outliers and more robust than $K$-means.
- can handle categorical features ($K$-means doesn't because it is based on calculating the cluster centers by taking the mean in each dimension).

<!-- - User also needs to specifiy $K$, which is the number of clusters. -->

## The PAM algorithm

1. **Initialization:** Randomly select $K$ data points as the medoids. 
2. **Assignment:** Assign each data point $x_i$ to its closest medoid $m$ and calculate the within-cluster variation for each medoid (by summing up the distances of the current medoid $m$ to all other data points associated to $m$) .
3. **Update:** Swap $m$ and $x_i$ and recompute the within-cluster variation to see if another medoid is more appropriate. Select the medoid $m$ with the lowest within-cluster variation.
4. Repeat steps (2) and (3) until medoids do not change.

## The PAM algorithm

The PAM algorithm typically uses the following two metrics to compute distances:

- The euclidean distance (root sum-of-squares of differences).
- The Manhattan distance (the sum of absolute distances).

**Note:** The Manhattan distance should give more robust results if your data contains outliers.
In all other cases, the results will be similar for both metrics.

## K-means vs. K-medoids

```{r, fig.width = 10, fig.height = 6, fig.align="center", out.width="\\textwidth", echo=FALSE, message=FALSE, results="asis"}
kclust = function(x, centers=NULL, k=NULL, alg="kmeans", maxiter=100) {
  n = nrow(x)
  p = ncol(x)
  if (is.null(centers)) {
    if (is.null(k)) stop("Either centers or k must be specified.")
    if (alg=="kmeans") centers = matrix(runif(k*p,min(x),max(x)),nrow=k)
    else centers = x[sample(n,k),]
  }
  k = nrow(centers)
  cluster = matrix(0,nrow=0,ncol=n)

  for (iter in 1:maxiter) {
    cluster.new = clustfromcent(x,centers,k)
    centers.new = centfromclust(x,cluster.new,k,alg)

    cluster = rbind(cluster,cluster.new)
    j = is.na(centers.new[,1])
    if (sum(j)>0) centers.new[j,] = centers[j,]
    centers = centers.new

    if (iter>1 & sum(cluster[iter,]!=cluster[iter-1,])==0) break
  }
  return(list(centers=centers,cluster=cluster[iter,],iter=iter,cluster.history=cluster))
}

# Compute clustering assignments, given centers
clustfromcent = function(x, centers, k) {
  n = nrow(x)
  dist = matrix(0,n,k)
  for (i in 1:k) {
    dist[,i] = colSums((t(x)-centers[i,])^2)
  }
  return(max.col(-dist,ties="first"))
}

# Compute centers, given clustering assignments
centfromclust = function(x, cluster, k, alg) {
  if (alg=="kmeans") return(avgfromclust(x,cluster,k))
  else return(medfromclust(x,cluster,k))
}

avgfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else centers[i,] = colMeans(x[j,,drop=FALSE])
  }
  return(centers)
}

medfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else {
      d = as.matrix(dist(x[j,],diag=T,upper=T)^2)
      ii = which(j)[which.min(colSums(d))]
      centers[i,] = x[ii,]
    }
  }
  return(centers)
}

# Compute within-cluster variation
wcv = function(x, cluster, centers) {
  k = nrow(centers)
  wcv = 0
  for (i in 1:k) {
    j = cluster==i
    nj = sum(j)
    if (nj>0) {
      wcv = wcv + sum((t(x[j,])-centers[i,])^2)
    }
  }
  return(wcv)
}

set.seed(1234)
pts = as.matrix(iris[, 3:4])

iters = 2
K = 3
km = kmed = vector("list", iters)
centers = kmed.centers = vector("list", iters + 1)

ind = sample.int(nrow(pts), K)
centers[[1]] = kmed.centers[[1]] = pts[c(99,93,91), , drop = FALSE]

par(mar = c(4,4,3,0))
plot(pts, pch = 19, col = "gray70")
points(centers[[1]][, 1], centers[[1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
title(main = "Iteration 0", line = 2)
title(main = "Choose K arbitrary observations to be the initial cluster centers", line = 1, cex.main = 0.8)

par(mfrow = c(1, 2))
for (i in 1:iters) {
  cat("  \n##", "K-means vs. K-medoids  \n  \\addtocounter{framenumber}{-1}  \n")
  # kmeans
  km[[i]] = kmeans(pts, centers = centers[[i]], nstart = 1, algorithm = "Lloyd", iter.max = 1)
  col = km[[i]]$cluster
  # kmedoids
  kmed[[i]] = kclust(pts, centers = kmed.centers[[i]], alg = "kmedoids", maxiter = 1)
  col.kmed = kmed[[i]]$cluster
  
  # kmeans
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  title(main = paste("K-means iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(centers[[i]][, 1], centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  
  # kmedoids
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col.kmed)
  title(main = paste("K-medoids iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(kmed.centers[[i]][, 1], kmed.centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  
  cat("  \n")
  cat("  \n##", "K-means vs. K-medoids  \n  \\addtocounter{framenumber}{-1}  \n")
  
  centers[[i + 1]] = km[[i]]$centers
  kmed.centers[[i + 1]] = medfromclust(pts, kmed[[i]]$cluster, K)
  
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(centers[[i + 1]][, 1], centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("K-means iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)
  
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(kmed.centers[[i + 1]][, 1], kmed.centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("K-medoids iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)
  # 
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), col = 1:3, pch = 19, cex = 2)
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), pch = 21, cex = 2)
  
  cat("  \n")
}
```

## $K$-medoids in R 

The $K$-medoids algorithm is implemented in the `pam` function included in the `cluster` R-package.

We compare $K$-means and $K$-medoids on a modified iris data with additional outliers:

```{r, echo = FALSE, eval = FALSE}
set.seed(5)
random = data.frame(Petal.Length = runif(2, 2.0, 3.5), Petal.Width = runif(2,1.0,10.5))
example = rbind(random, iris[,3:4])
```

```{r, echo = TRUE}
library(cluster)
# add outliers to iris
out = data.frame(Petal.Length = c(2.3, 3), Petal.Width = c(9.7, 3.7))
iris2 = rbind(iris[, 3:4], out)

# K-medoid vs. K-means
kmedoid = pam(iris2, k = 3)
km = kmeans(iris2, centers = 3, iter.max = 100, nstart = 100)
```

## $K$-medoids in R 

```{r, fig.width = 10, fig.height = 6, fig.align="center", out.width="\\textwidth", echo=FALSE}
set.seed(5)
par(mfrow = c(1,2), mar = c(4,3,3,0))

plot(iris2, col=kmedoid$clustering, pch=19, cex=1, main = "K-Medoids")
points(kmedoid$medoids, col="red", pch=4, cex=3, lwd=3)

plot(iris2, col = km$cluster, pch = 19, cex = 1, main = "K-Means")
points(km$centers, col="red", pch=4, cex=3, lwd=3)
```

## Summary

- Minimizing the *within-cluster variation* exactly is not feasible and can be approximated by the $K$-means algorithm.

- $K$-means always converges, however, the cluster assignments strongly depend on the initial
  centers. \
  $\rightarrow$ repeat it several times with different initial centers.

- A simple solution for choosing the number of clusters $K$ is to plot the
  *within-cluster variation* for several $K$ and look for an *"elbow"* which is a good guess for $K$.
  
