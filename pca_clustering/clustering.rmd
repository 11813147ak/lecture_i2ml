---
title: Day 2 - Unsupervised Methods
subtitle: Lecture 2 - Clustering
author: Giuseppe Casalicchio M.Sc.
date: 05.02.2019
output:
  beamer_presentation:
    includes:
      in_header: "../../_setup/preamble_munichre.sty"
    template: "../../_setup/custom_pandoc.tex"
---

## {.plain}

```{r, child="../../_setup/setup.Rmd",echo=FALSE,message=FALSE,include=FALSE}
```

<!-- This file was created automatically! Please check the entrys! -->



# Hierarchical Clustering

## Motivation for Clustering

Consider multivariate data with $N$ observations (e.g. customers) and $P$ 
features (e.g. characteristics of customers).

Task: divide data into groups (clusters), such that 

- the observations in each cluster are as "similar" as possible (homogeneity 
  within each cluster), and

- the clusters are as "far away" as possible from other clusters (heterogeneity 
  between different clusters).

## Clustering vs. Classification

- In classification, the groups are known and we try to learn what 
  differentiates these groups (i.e., learn a classification function) to 
  properly classify future data.

- In clustering, we look at data, where groups are unknown and try to find 
  similar groups.

Why do we need clustering?

- Discovery: looking for new insights in the data (e.g. finding groups of customers that buy a similar product).

- Derive a reduced representation of the full data set.











## Hierarchical Clustering
```{r include=FALSE, cache=FALSE}
library(knitr)
root = rprojroot::find_root(rprojroot::is_git_root)
ap = function (myfile) paste0(file.path(readSlide("hierarchical_clustering_distances", file = FALSE), "/figure", myfile))
```

Hierarchical clustering is a recursive process that builds a hierarchy of clusters. 
We distinguish between:

1. Agglomerative (or bottom-up) clustering:
    - Start: Each observations is an *individual cluster*.
    - Repeat: Merge the two closest clusters.
    - Stop when there is only one cluster left.

2. Divisive (or top-down) clustering:
    - Start: All observations are within *one* cluster.
    - Repeat: Divide the cluster that results in two clusters with biggest distance.
    - Stop when each observation is an individual cluster.
    
<!-- We focus on agglomerative clustering methods as they are simpler. -->

## Hierarchical Clustering

Let $X_1,\hdots , X_N$ be observations with $P$ features (dimensions), where 
$X_i = (x_{i1}, \ldots, x_{iP})^\top$. A data set is a (N $\times$ P)-matrix of 
the form:

|        | feature $1$ | $\hdots$   | $\hdots$    | feature $P$|
|:------:|:-----------:|:----------:|:-----------:|:----------:|
|$X_1$   |     $x_{11}$|    $\hdots$|     $\hdots$|    $x_{1P}$|
|$\vdots$|     $\vdots$|    $\vdots$|     $\vdots$|    $\vdots$|
|$X_N$   |     $x_{N1}$|    $\hdots$|     $\hdots$|    $x_{NP}$|
 
<!-- |$\vdots$|     $\vdots$|    $\vdots$|     $\vdots$|    $\vdots$| -->
<!-- |$X_{150}$|         5.9|         3.0|          5.1|         1.8| -->

## Hierarchical Clustering 

For hierarchical clustering, we need a definition for

- distances $d(X_i, X_j)$ between two observations $X_i$ and $X_j$:

    - manhattan distance: 
      $$d(X_i,X_j)= ||X_i - X_j||_1 = \sum_{k=1}^P|x_{ik}-x_{jk}|$$
    - euclidean distance:
      $$d(X_i,X_j)= ||X_i - X_j||_2 = \sqrt{\sum_{k=1}^P(x_{ik}-x_{jk})^2}$$

- distances between two clusters (called linkage).

<!-- 
 -  Let $K$ be the number of clusters.
 - A clustering of observations $X_1,\hdots , X_N$ is a function $C$ that 
   assigns each observation $X_i$ to a cluster $k \in \{1,\hdots , K\}$. 
-->

## Distances between Observations

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
```

```{r,echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.5\\textwidth"}
getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}

par(mar = c(4,4,0,0))
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19, 
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(X[i]), expression(X[j])), adj = c(1.5, 0))
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = 2)
legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(2,1))

text(x = 5, y = 1, expression(d(X[i],X[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = 2)

asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(X[i],X[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
```


- \small manhattan: sum up the absolute distances in each dimension.

    In R: `dist(data, method = "manhattan")`
    
- \small euclidean: remember Pythagoras theorem from school?

    In R: `dist(data, method = "euclidean")`

- \small gower: can be used for mixed variables (categorical and numeric).

    In R: `gower_dist()` from the `gower` package

- \small see `?dist` for other distances.

## Gower Distance I

- The Gower's metric calculates the distance between observations $X_i$ and $X_j$ for each feature separately and based on its data type (i.e., categorical or numeric).

- For a categorical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by
  $$s_{ijk}=\begin{cases} 0\ \text{if}\ X_{ik}=X_{jk} \\ 1\ \text{if}\ X_{ik}\neq X_{jk}.\end{cases}$$

- For a numerical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by $$s_{ijk}=\frac{|X_{ik}-X_{jk}|}{\max(X_k)-\min(X_k)}, \;\;\; \text{so that } 0 \leq s_{ijk} \leq 1.$$ 

## Gower Distance II

The Gower's metric $S$ combines all individual distances of each feature by
$$S_{ij}=\dfrac{\sum_{k=1}^P w_{k} s_{ijk}}{\sum_{k=1}^P w_{k}},$$ where
      
  - $P$: number of features.
  - $w_k$: weight for feature $k$ (typically $w_k = 1$).
  - $s_{ijk}$: the difference (distance) between $X_{ik}$ and $X_{jk}$, i.e. the $i$-th and $j$-th observation of feature $k$.

  
## Gower Distance III - Example      

`r include_graphics(ap("gower_distance.png"))`


## Distances between Observations

It is often a good idea to *normalize* the data before computing distances, 
especially when the scale of features is different, e.g.:

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=9}
par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0))

dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)", 
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))


dat$height = dat$height/100
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)", 
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))

```

- On the right plot, the distance is dominated by `shoe size`.

## Distances between Observations

\small
The normalized feature $\tilde{X}_{\texttt{height}}$ is computed using 
$X_{\texttt{height}}$ by <!-- Normalization of the \texttt{height} feature means: -->
\[\tilde{X}_{\texttt{height}} = \tfrac{X_{\texttt{height}}-\texttt{mean}(X_{\texttt{height}})}{\texttt{sd}(X_{\texttt{height}})}.\]

Distances based on normalized data are better comparable and robust in terms of 
linear transformations (e.g. unit conversion).

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.55\\textwidth"}
par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0))

#dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
dat = as.data.frame(scale(dat))
plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height", 
  pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))), 
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))

```

## Distances between Clusters (Linkage)

- Assume that all observations $X_1,\hdots , X_N$ belong to $K<N$ different clusters.

- The linkage of two clusters $C_r$ and $C_s$ is a "score" describing their distance.
  
The most popular and simplest linkages are

- *Single Linkage* <!-- the shortest distance between two observations in each cluster.  -->

- *Complete Linkage* <!-- the longest distance between two observations in each cluster. -->

- *Average Linkage* <!-- the average distance between each observation in one cluster to every observation in the other cluster. -->

- *Centroid Linkage*

```{r, echo=FALSE, message=FALSE}
library(MASS)
library(cluster)
library(pdist)
set.seed(12345)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,2), Sigma = diag(2))
cl2 = mvrnorm(n = n, mu = c(1,-3), Sigma = diag(2)*2)
e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
dat = rbind(cl1, cl2)
d = as.matrix(pdist(cl1, cl2))
```

## Single Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
single = which(d==min(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[single[1],], cl2[single[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

Single linkage defines <!-- we compute all pairwise distances for observations in 
opposite clusters.--> the distance of the *closest point pairs* from different clusters as the distance between two clusters:

\[d_{\text{single}}(C_r,C_s) = \min_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]

## Complete Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
complete = which(d==max(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[complete[1],], cl2[complete[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

Complete linkage defines the distance of the *furthest point pairs* of different clusters as 
the distance between two clusters:

\[d_{\text{complete}}(C_r,C_s) = \max_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]

## Average Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

for(i in 1:nrow(cl2)) lines(rbind(cl1[single[1],], cl2[i,]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

\begin{center}
{\scriptsize (Note: Plot only shows distances between all green points and 
\textit{one} red point)}
\end{center}

In average linkage, the distance between two clusters is defined as the average 
distance across *all* pairs of two different clusters.

## Centroid Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.65\\textwidth"}
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)#, adj = c(0, 0.5))
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)#, adj = c(1, 0.5))


#points(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), pch = 1, cex = 4)
lines(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), type = "b", cex = 3.5)
```

\small
Centroid linkage defines the distance between two clusters as the distance between the two cluster centroids.
The centroid of a cluster $C_s$ with $N_s$ points is the mean value of each dimension:

\[\bar{X}_s = \frac{1}{N_s} \sum_{i \in C_s} X_i\]









## Example: Hierarchical Clustering

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$ \\
% Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$\\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$ \\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{red}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
lines(simple.data[4:5,], col = "red")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: ${\color{blue}\{1,2,3\}},{\color{red}\{4,5\}}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1:3, 1),], col = "blue")
lines(simple.data[4:5,], col = "red")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: $\{1,2,3\},\{4,5\}$\\
Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1,2,3,5,4,1),], col = "blue")
```

\end{column}
\end{columns}

## Dendrogram

A Dendrogram is a tree showing which clusters / observations are merged after each step.
The `height' is proportional to the distance between the two merged clusters: 

```{r, echo = FALSE, results='hide'}
# compute distance matrix
euclid = dist(simple.data, method = "euclidean")
# do clustering with complete linkage
agglo = hclust(euclid, method = "complete")
agglo
```

```{r, echo=FALSE, out.width="\\textwidth", fig.width=8, fig.height=4}
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))

par(mar = c(2,3,1,4))
plot(agglo, hang = -1, ylab = "", axes = F)
axis(2, line = 0, padj = 0.5)
title(ylab = "Height", line = 2)
abline(h = c(0, agglo$height), lty = 2, col = "gray70")
text(x = 5, y = c(0, agglo$height), labels = paste("Step ", 1:5), xpd = TRUE, col = "gray70", pos = 4)
```

<!-- # Dendrogram -->

<!-- A Dendrogram is a tree that visualizes the merging of clusters: -->

<!-- - Each leaf node is a singleton (i.e., a cluster containing a single  -->
<!--   observation). -->

<!-- - Each internal node has two daughter nodes (children), representing -->
<!--   the clusters that were merged to form it.  -->

<!-- - Each node represents a cluster. -->

<!-- - The root node is the cluster containing all observations. -->

<!-- - The $y$ axis shows the distance between two clusters when they were merged. -->

<!-- Remember: the choice of linkage determines how the distance between clusters is  -->
<!-- measured.  -->

<!-- If we fix the leaf nodes at height zero, then each internal node is
drawn at a height proportional to the dissmilarity between
its two daughter nodes -->










## R Example with Iris Data

The data contains 150 leaf measurements for 3 flower species:

```{r, eval = FALSE}
pairs(iris[1:4], col = iris$Species)
```

```{r, echo=FALSE, out.width="0.9\\textwidth", fig.align="center", fig.height = 5.5, fig.width=9}
pairs(iris[1:4], pch = 19, col = iris$Species, oma = c(2,2,2,12))
legend(0.95, 1, levels(iris$Species), xpd = TRUE, horiz = F, xjust = 0.5,
       fill = unique(iris$Species), title = "Species")
```

## R Example with Iris Data

We now "forget" the real groups specified by the `Species` variable and try to find clusters based on the leaf measurements. 
<!--  Clustering techniques essentially try to formalise what human observers do 
so well in two or three dimensions (grouping by eye). -->

```{r, echo=FALSE, out.width="0.8\\textwidth", fig.align="center"}
pairs(iris[1:4], pch = 19)
```

## R Example with Iris Data

```{r, eval=FALSE}
# compute distance matrix
d.euclid = dist(iris[1:4], method = "euclidean")
# do clustering with average linkage
cl = hclust(d.euclid, method = "average")
plot(cl, labels = FALSE, hang = -1) # plot dendrogram
rect.hclust(cl, k = 3) # highlight the k = 3 groups
```

```{r, echo=FALSE, fig.width=12, fig.height=5}
par(mar = c(1,4,1,0))
d.euclid = dist(iris[1:4], method = "euclidean")
cl = hclust(d.euclid, method = "average")
plot(cl, labels = FALSE, hang = -1)
rect.hclust(cl, k = 3)
```

## R Example with Iris Data

We can extract the clustering assignments by cutting the dendrogram, e.g. using $k=3$ clusters:

```{r, eval = FALSE}
group = cutree(cl, k = 3) # get clusters assignments for k=3
pairs(iris[1:4], col = group) # plot clusters with different colors
```

```{r, out.width="0.75\\textwidth", fig.width=8, fig.height=5, fig.align="center", echo = FALSE}
group = cutree(cl, k = 3) 
pairs(iris[1:4], pch = 19, col = group) 
```









## Properties: Single Linkage

```{r, echo=FALSE}
#Colors = colorspace::rainbow_hcl(3)
set.seed(0)
x = rbind(
  scale(matrix(rnorm(2*20, sd = 0.4), ncol = 2), center = c(1,1), scale = F),
  scale(matrix(rnorm(2*30, sd = 0.4), ncol = 2), center = -c(1,1), scale = F)
  )
x = rbind(x, matrix(rep(seq(-1,1, length.out = 10), each = 2), ncol = 2, byrow = TRUE))
#x = rbind(x, matrix(runif(2*10, min(x), max(x)), ncol=2))
d = dist(x)

tree.sing = hclust(d,method = "single")
tree.comp = hclust(d,method = "complete")
tree.avg = hclust(d,method = "average")
tree.cen = hclust(d,method = "centroid")

k = 3

labs.sing = cutree(tree.sing, k = k)
labs.comp = cutree(tree.comp, k = k)
labs.avg = cutree(tree.avg, k = k)
labs.cen = cutree(tree.cen, k = k)
```

Single linkage introduces the *chaining problem*:

  - Only one pair of points needs to be close to merge clusters.
  - A chain of points can expand a cluster over long distances.
  - Points within a cluster can be too widely spread and not dense enough.

```{r, echo = FALSE, fig.height=4, out.width="0.7\\textwidth"}
par(mfrow=c(1,2), mar = c(3,3,2,2))

plot(x,col=Colors[labs.sing], pch = 19, ylab = "", xlab = "", main = "single (chaining effect)")
plot(tree.sing, labels=F, hang=-1)
rect.hclust(tree.sing, k = k)#, border = Colors[1])

# http://www.sthda.com/english/articles/28-hierarchical-clustering-essentials/92-visualizing-dendrograms-ultimate-guide/
#fviz_dend(tree.sing, cex = 0.5, k = 3, # Cut in four groups
#  k_colors = "jco", rect = TRUE, rect.fill = TRUE, rect_border = "jco", lower_rect = 0)
```

## Properties: Complete Linkage

Complete linkage avoids chaining, but suffers from *crowding*: 

  - Merging is based on the furthest distance of point pairs from different clusters.
  - Points of two different clusters can thus be closer than points within a cluster. 
  - Clusters are dense, but too close to each other. <!-- and sensitive to outliers. -->

```{r, echo = FALSE, fig.height=4, out.width="0.7\\textwidth"}
par(mfrow=c(1,2), mar = c(3,3,2,2))

plot(x,col=Colors[labs.comp], pch = 19, ylab = "", xlab = "", main = "complete (crowding effect)")
plot(tree.comp, labels=F, hang=-1)
rect.hclust(tree.comp, k = k)#, border = Colors[1])

# http://www.sthda.com/english/articles/28-hierarchical-clustering-essentials/92-visualizing-dendrograms-ultimate-guide/
#fviz_dend(tree.sing, cex = 0.5, k = 3, # Cut in four groups
#  k_colors = "jco", rect = TRUE, rect.fill = TRUE, rect_border = "jco", lower_rect = 0)
```


## Properties: Average Linkage

- Average linkage is based on the average distance between clusters and tries to avoid crowding and chaining.
- Produces clusters that are quite dense and rather far apart.

```{r, echo=FALSE, fig.height=4.25, fig.width=7, out.width="0.8\\textwidth"}
par(mfrow=c(2,3), mar = c(3,3,2,2))

plot(x,col=Colors[labs.sing], pch = 19, ylab = "", xlab = "", main = "single (chaining effect)")
plot(x,col=Colors[labs.comp], pch = 19, ylab = "", xlab = "", main = "complete (crowding effect)")
plot(x,col=Colors[labs.avg], pch = 19, ylab = "", xlab = "", main = "average")
#plot(x,col=Colors[labs.cen], pch = 19, ylab = "", xlab = "", main = "centroid")

par(mar = c(1,4,2,1))
plot(tree.sing, labels=F, hang=-1)
rect.hclust(tree.sing, k = k)#, border = Colors[1])
plot(tree.comp, labels=F, hang=-1)
rect.hclust(tree.comp, k = k)#, border = Colors[1])
plot(tree.avg, labels=F, hang=-1)
rect.hclust(tree.avg, k = k)#, border = Colors[1])
#plot(tree.cen, labels=F, hang=-1)
#rect.hclust(tree.cen, k = k)#, border = Colors[1])
```

<!--
Average linkage isn’t perfect, it has its own problems:

- It is not clear what properties the resulting clusters
  have when we cut an average linkage tree at given height $h$. Single
  and complete linkage trees each had simple interpretations.

- Results of average linkage clustering can change with a
  monotone increasing transformation of the distances.
  <!-- $d_{ij}$. I.e., if $h$ is such that $h(x) \leq h(y)$ whenever
  $x \leq y$, and we used dissimilarites $h(d_{ij})$ instead of
  $d_{ij}$, then we could get different answers 
  Depending on the context, this problem may be important or unimportant. 
  E.g., it could be very clear what distances should be used, or not. 

Note: results of single and complete linkage clustering are
unchanged under monotone transformations. (Exercise) -->

<!--
## Example of a change with monotone increasing transformation

![image](mono1.pdf) ![image](mono2.pdf)

\[fragile\]

## Hierarchical agglomerative clustering in R

The function hclust in the base package performs
hierarchical agglomerative clustering using single, complete, or average
linkage

E.g.,

    d = dist(x)
    tree.avg = hclust(d, method="average")
    plot(tree.avg)
-->

## Properties: Centroid Linkage

- Centroid linkage defines the distance based on **artificial data points** (the cluster centers), which produces dendrograms **with inversions**, i.e., the distance between the clusters to be merged can be smaller in the next step.

- In single, complete and average linkage, the distance between the clusters to be merged increases in each step. \
$\Rightarrow$ always produces dendrograms **without inversions**.

```{r, echo=FALSE, fig.width = 8, fig.height = 3.5, out.width="0.7\\textwidth"}
simple.data = data.frame(
  x = c(0,1,0.5),
  y = c(0,0,0.9)
)

#simple.data = as.data.frame(rbind(c(1,1), c(5,1), c(3,1+2*sqrt(3))))
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), 
  xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data[1,], labels = 1, pos = 3)
text(simple.data[2,], labels = 2, pos = 3)
text(simple.data[3,], labels = 3, pos = 1)
centroid = colMeans(simple.data[1:2,])
points(centroid[1], centroid[2], col = "gray60", pch = 19)
text(centroid[1], centroid[2], labels = expression(C[12]), col = "gray60", pos = 4)
centroid = colMeans(simple.data[1:3,])
#points(centroid[1], centroid[2], col = "gray60", pch = 19)
#text(centroid[1], centroid[2], labels = expression(C[123]), col = "gray60", pos = 4)

agglo = hclust(dist(simple.data, method = "euclidean")^2, method = "centroid")
#cophenetic(agglo)
agglo$height = sqrt(agglo$height)

par(mar = c(2,4,1,1))
plot(as.dendrogram(agglo), main = "Cluster Dendrogram", ylab = "Height", ylim = c(0,1))
#rect.hclust(agglo, k = 2)
```

<!-- ## Properties -->

<!-- - Results of average and centroid linkage can change with a monotone increasing  -->
<!--   transformation of the distances, e.g. using the *squared euclidean distance*  -->
<!--   instead of the euclidean distance.  -->
<!-- - Results of single and complete linkage are insensitive to monotone increasing  -->
<!--   transformation of the distances. -->
<!-- - Centroid linkage produces inversions as it measures the distance based on *artificial data points* (cluster centers). -->
<!-- - In single, complete and average linkage there are no inversions. -->

  <!-- \newline
  Note: Results of single and complete linkage clustering will be unchanged. -->
    









## Summary

- *Hierarchical agglomerative clustering methods* iteratively merge observations/clusters until all observations are in one single cluster.

- Results in a hierarchy of clustering assignments which can be visualized in a *dendrogram*. 
Each node of the dendrogram represents a cluster and its `height' is proportional to the distance of its child nodes.

- The most common linkage functions are *single, complete*, *average* and *centroid* linkage. 
There is no perfect linkage and each linkage has its own advantages and disadvantages.



# Partitioning Clustering Methods








## Optimal Partitioning Clustering

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2, 
                     function(x) 
                       rgb(x[1], x[2], x[3], alpha=alpha))  
}
```

**Hierarchical clustering:** 
Stepwise merging (agglomerative methods) or dividing (divisive methods) of clusters based on distances and linkages. 
The number of clusters are selected by splitting the dendrogram at a specific threshold for the ``height'' after visual inspection.

**Partitioning clustering:** 
Partitions the $N$ observations into a predefined number of $K$ clusters by optimizing a numerical criterion. The most common partitioning methods are:

- $K$-means
- $K$-medians
- $K$-medoids (Partitioning Around Medoids (PAM))

## $K$-means

$K$-means partitions the $N$ observations into $K$ predefined clusters $C_1, C_2, \hdots, C_K$ by minimizing the **compactness**, i.e. the **within-cluster variation** of all clusters using
$$\textstyle\sum_{k=1}^K \sum_{i \in C_k} \|X_i-\bar{X}_k\|_2^2 \rightarrow \min,$$ 
where $\bar{X}_k = \frac{1}{N_k} \sum_{i \in C_k} X_i$ is the centroid of cluster $k$ and $N_k$ is the number of observations in cluster $k$. 

<!--Hence, equivalently we seek a clustering $C$ that minimizes the 
within-cluster variation.-->

```{r, echo = FALSE, fig.height=3.5, fig.width=6.5, out.width="0.6\\textwidth", message=FALSE}
Colors = colorspace::rainbow_hcl(3)
library(MASS)
library(cluster)
library(pdist)
set.seed(123456)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,0), Sigma = diag(2)*0.2)
cl2 = mvrnorm(n = n, mu = c(3,-1.5), Sigma = diag(2)*0.3)
cl3 = mvrnorm(n = n, mu = c(2,0.25), Sigma = diag(2)*0.6)

e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
e3 = ellipsoidhull(cl3)
dat = rbind(cl1, cl2, cl3)
d1 = as.matrix(pdist(cl1, cl2))
d2 = as.matrix(pdist(cl1, cl3))
d3 = as.matrix(pdist(cl2, cl3))

par(mar = c(0,0,0,0))

pr = (rbind(predict(e1), predict(e2), predict(e3)))
plot(dat, pch = 19, col = rep(Colors, each = n),
  xlab = "", ylab = "", axes = F, xlim = range(pr[,1]), ylim = range(pr[,2]))
lines(predict(e1), col = Colors[1])
lines(predict(e2), col = Colors[2])
lines(predict(e3), col = Colors[3])


centers = as.data.frame(rbind(
  c("x" = e2$loc[1], "y" = e2$loc[2]), 
  c("x" = e1$loc[1], "y" = e1$loc[2])))
#lines(centers, col = "black", lwd = 2)
arrows(x0 = centers$x[1], x1 = centers$x[2], 
  y0 = centers$y[1], y1 = centers$y[2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(centers)
text(xy[1], xy[2], label = "between cluster variation", col = "black")

text(e1$loc[1], e1$loc[2], label = expression(C[1]), col = Colors[1], pos = 1)
text(e2$loc[1], e2$loc[2], label = expression(C[2]), col = Colors[2], pos = 1)
text(e3$loc[1], e3$loc[2], label = expression(C[3]), col = Colors[3], pos = 3)

d = predict(e1)
#summary(d)
xy = dat[c(14,16),]
#lines(xy, col = "black", lwd = 2)
arrows(x0 = xy[1,1], x1 = xy[2,1], 
  y0 = xy[1,2], y1 = xy[2,2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(xy)
text(xy[1], xy[2], label = "within-cluster variation", col = "black")

```


## $K$-means

Idea: Consider every possible partition of $N$ observations into $K$ clusters and select the one with the lowest **within-cluster variation**.

Problem: Requires trying all possible assignments of $N$ observations into $K$ clusters, which in practice is nearly impossible (Hothorn et al., 2009, p.322):

|$N$     |$K$     | Number of possible partitions|
|-------:|:------:|:-----------------------------|
|15      | 3      | 2.375.101                    |
|20      | 4      | 45.232.115.901               |
|100     | 5      | $10^{68}$                    |

\tiny{Hothorn, T., Everitt, B. S. (2009). A handbook of statistical analyses using R. Chapman and Hall/CRC.}

## $K$-means
<!--
The $K$-means clustering algorithm approximately minimizes
the enlarged criterion by alternately minimizing over $C$
and $c_1,\hdots , c_K$

We start with an initial guess for $c_1,\hdots , c_K$ (e.g., pick $K$
points at random over the range of $X_1,\hdots , X_n$), then repeat:

1.  Minimize over $C$: for each $i=1,\hdots , n$, find the
    cluster center $c_k$ closest to $X_i$, and let $C(i)=k$

2.  Minimize over $c_1,\hdots , c_K$: for each
    $k=1,\hdots , K$, let $c_k = \bar{X}_k$, the average of points in
    group $k$

Stop when within-cluster variation doesn’t change

In words:

1.  Cluster (label) each point based the closest center

2.  Replace each center by the average of points in its cluster

## $K$-means example

Here $X_i \in \R^2$, $n=300$, and $K=3$ -->

<!-- ![image](km0.pdf) -->

<!-- ![image](km1.pdf) -->

<!-- ![image](km2.pdf)\ -->
<!-- ![image](km3.pdf) -->

<!-- ![image](km9.pdf) --> 

Use an approximation:

1. **Initialization:** Choose $K$ arbitrary observations to be the initial cluster 
   centers.

2. **Assignment:** Assign every observation to the cluster with the closest center.

3. **Update:** Compute the new center of each cluster as the mean of its members.

4. Repeat (2) and (3) until the centers do not move.
 
<!--
1. Find some initial partition of the observations into the required number of clusters. 
2. Calculate the change in the clustering criterion produced by "moving" each 
individual from its own cluster to another cluster.
3. Make the change that leads to the greatest improvement in the value of the clustering criterion.
4. Repeat steps (2) and (3) until no move of an observation causes the clustering criterion to improve.
-->
<!-- Cool Example: http://www.edureka.co/blog/implementing-kmeans-clustering-on-the-crime-dataset/ -->










## $K$-means Example

```{r, fig.width = 7, fig.height = 5, fig.align="center", echo=FALSE, message=FALSE, results="asis"}
set.seed(1234)
pts = iris[, 3:4]

iters = 3
K = 3
km = vector("list", iters)
centers = vector("list", iters + 1)

centers[[1]] = pts[sample.int(nrow(pts), K), , drop = FALSE] #matrix(c(1,0.5,2,1,7,2), ncol = 2, byrow = TRUE)

par(mar = c(4,4,3,0))
plot(pts, pch = 19, col = "gray70")
points(centers[[1]][, 1], centers[[1]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
title(main = "Iteration 0", line = 2)
title(main = "Choose K arbitrary observations to be the initial cluster centers", line = 1, cex.main = 0.8)
  
for (i in 1:iters) {
  cat("  \n##", "$K$-means Example  \n  \\addtocounter{framenumber}{-1}  \n")
  km[[i]] = kmeans(pts, centers = centers[[i]], nstart = 1, algorithm = "Lloyd", iter.max = 1)
  
  col = km[[i]]$cluster
  plot(pts, pch = 19, col = col)
  title(main = paste("Iteration", i), line = 2)
  title(main = "Assign observations to nearest cluster center", line = 1, cex.main = 0.8)
  points(centers[[i]][, 1], centers[[i]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
  
  cat("  \n")
  cat("  \n##", "$K$-means Example  \n  \\addtocounter{framenumber}{-1}  \n")
  
  centers[[i + 1]] = km[[i]]$centers
  plot(pts, pch = 19, col = col)
  points(centers[[i + 1]][, 1], centers[[i + 1]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
  title(main = paste("Iteration", i), line = 2)
  title(main = "Compute new cluster center", line = 1, cex.main = 0.8)
  cat("  \n")
}
```

<!-- Select 3 random points and assume them to be cluster centers for the clusters to be created. -->

## $K$-means in R

The $K$-means algorithm is part of the base distribution in R, given by
the `kmeans` function (using `algorithm = "Lloyd"`):

```{r, echo = TRUE, size = "tiny"}
km = kmeans(iris[,3:4], centers = 3, nstart = 100, iter.max = 100, algorithm = "Lloyd")
str(km)
```

## $K$-means in R

The final cluster assignments can be visualized by

```{r, fig.height = 5, out.width = "\\textheight", fig.align="center", echo = 2}
par(mar = c(4,4,1,1))
plot(iris[,3:4], pch = 19, col = km$cluster)
```









## Properties of $K$-means

- $K$-means is based on computing the mean, which is sensitive to outliers and can only be computed for numerical data.

- The **within-cluster variation** is reduced in each iteration. 
  In R, the maximum number of iterations is specified by `iter.max`.

- The final result is typically not the best result that globally minimizes the **within-cluster variation**. \
  $\rightarrow$ would only be possible after trying all possible partitions!

- $K$-means can be restarted multiple times using `nstart`.
  The clustering with the smallest within-cluster variation is then selected as 
  the best solution.
  
## Properties of $K$-means

- $K$-means produces different clusters depending on the initial centers and always converges, e.g.:
<!-- In fact, it takes $\leq K^n$ iterations (why?) -->

```{r, echo=FALSE, fig.width=8, fig.height=4, fig.align="center", out.width="\\textwidth"}
set.seed(0)
x = rbind(
  scale(matrix(rnorm(2*20, sd = 0.4), ncol=2), center=c(1,1), scale = F),
  scale(matrix(rnorm(2*30, sd = 0.4), ncol=2), center=-c(1,1), scale = F)
  )
x = rbind(x, matrix(rep(seq(-1,1, length.out = 10), each = 2), ncol = 2, byrow = TRUE))
#x = rbind(x, matrix(runif(2*10, min(x), max(x)), ncol=2))
d = dist(x)

init.col = add.alpha("#000000", alpha = 0.5)
par(mfrow=c(1,3), mar = c(3,3,1,1))
for(i in 1:3) {
  rand = c(1, 25, 54)+i
  km1 = kmeans(x, centers = x[rand, ], nstart = 1, algorithm = "Lloyd")
  plot(x, pch = 19, col = km1$cluster, ylim = c(-1.5, 2))
  points(x[rand, 1], x[rand, 2], pch = 4, cex = 2, lwd = 2, col = init.col)
  if(i == 1) legend("topleft", pch = 4, pt.lwd = 2, pt.cex = 2, legend = "Initial Cluster Centers",
    col = init.col)
}

```

## Choice of $K$

- Many methods exist for choosing the number of clusters $K$ (there is no perfect solution).

- The easiest method is to apply $K$-means for different $K$ and plot the **within-cluster variation** for each number of $K$.
  
- The **within-cluster variation** always decreases with increasing number of clusters. 

- An **"elbow"** in the plot might indicate a useful solution.

```{r, echo=FALSE, fig.height=4.5, fig.align="center", out.width="0.7\\textheight"}
set.seed(1)
K = 8
wss = numeric(K)
for (k in 1:K) {
  km = kmeans(iris[,3:4], centers = k, nstart = 10, iter.max = 100, algorithm = "Lloyd")
  wss[k] = km$tot.withinss
}
par(mar = c(4,4,1,1))
plot(1:K, wss, type = "b", xlab = "K (number of clusters)", ylab = "within-cluster variation")
```









## $K$-medoids 

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2, 
                     function(x) 
                       rgb(x[1], x[2], x[3], alpha=alpha))  
}
```

$K$-medoids clustering 

- is strongly related to $K$-means and is realized by the **Partitioning Around Medoids (PAM)** algorithm.
- uses cluster medoids as representative clusters, i.e. **real data points** instead of **artificial data points** (such as the cluster centers as in $K$-means) are used. 
- is less sensitive to outliers and more robust than $K$-means.
- can handle categorical features ($K$-means doesn't because it is based on calculating the cluster centers by taking the mean in each dimension).

<!-- - User also needs to specifiy $K$, which is the number of clusters. -->

## The PAM algorithm

1. **Initialization:** Randomly select $K$ data points as the medoids. 
2. **Assignment:** Assign each data point $x_i$ to its closest medoid $m$ and calculate the within-cluster variation for each medoid (by summing up the distances of the current medoid $m$ to all other data points associated to $m$) .
3. **Update:** Swap $m$ and $x_i$ and recompute the within-cluster variation to see if another medoid is more appropriate. Select the medoid $m$ with the lowest within-cluster variation.
4. Repeat steps (2) and (3) until medoids do not change.

## The PAM algorithm

The PAM algorithm typically uses the following two metrics to compute distances:

- The euclidean distance (root sum-of-squares of differences).
- The Manhattan distance (the sum of absolute distances).

**Note:** The Manhattan distance should give more robust results if your data contains outliers.
In all other cases, the results will be similar for both metrics.

## K-means vs. K-medoids

```{r, fig.width = 10, fig.height = 6, fig.align="center", out.width="\\textwidth", echo=FALSE, message=FALSE, results="asis"}
# Compute either K-means clustering or K-medoids clustering
#
# Arguments:
# x: data matrix, n observations (rows) by p features (cols).
# centers: a vector giving the starting centers. Defaults
#   to NULL in which case we choose k centers at random.
# k: number of clusters. If the centers argument is specified,
#   then this doesn't need to be specified.
# alg: algorithm. Can be either "kmeans" or "kmedoids". 
#   Defaults to "kmeans".
# maxiter: Maximum number of iterations before we quit. 
#   Defaults to 100.
#
# Returns:
# centers: a vector of length k, giving the final centers.
# cluster: a vector of length n, giving the final clustering
#   assignments.
# iter: number of iterations performed.
# cluster.history: a matrix of dimension iter by k, each row
#   giving the cluster assignments at the corresponding iteration.
# 
kclust = function(x, centers=NULL, k=NULL, alg="kmeans", maxiter=100) {
  n = nrow(x)
  p = ncol(x)
  if (is.null(centers)) {
    if (is.null(k)) stop("Either centers or k must be specified.")
    if (alg=="kmeans") centers = matrix(runif(k*p,min(x),max(x)),nrow=k)
    else centers = x[sample(n,k),]
  }
  k = nrow(centers)
  cluster = matrix(0,nrow=0,ncol=n)

  for (iter in 1:maxiter) {
    cluster.new = clustfromcent(x,centers,k)
    centers.new = centfromclust(x,cluster.new,k,alg)

    cluster = rbind(cluster,cluster.new)
    j = is.na(centers.new[,1])
    if (sum(j)>0) centers.new[j,] = centers[j,]
    centers = centers.new

    if (iter>1 & sum(cluster[iter,]!=cluster[iter-1,])==0) break
  }
  return(list(centers=centers,cluster=cluster[iter,],iter=iter,cluster.history=cluster))
}

# Compute clustering assignments, given centers
clustfromcent = function(x, centers, k) {
  n = nrow(x)
  dist = matrix(0,n,k)
  for (i in 1:k) {
    dist[,i] = colSums((t(x)-centers[i,])^2)
  }
  return(max.col(-dist,ties="first"))
}

# Compute centers, given clustering assignments
centfromclust = function(x, cluster, k, alg) {
  if (alg=="kmeans") return(avgfromclust(x,cluster,k))
  else return(medfromclust(x,cluster,k))
}

avgfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else centers[i,] = colMeans(x[j,,drop=FALSE])
  }
  return(centers)
}

medfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else {
      d = as.matrix(dist(x[j,],diag=T,upper=T)^2)
      ii = which(j)[which.min(colSums(d))]
      centers[i,] = x[ii,]
    }
  }
  return(centers)
}

# Compute within-cluster variation
wcv = function(x, cluster, centers) {
  k = nrow(centers)
  wcv = 0
  for (i in 1:k) {
    j = cluster==i
    nj = sum(j)
    if (nj>0) {
      wcv = wcv + sum((t(x[j,])-centers[i,])^2)
    }
  }
  return(wcv)
}

set.seed(1234)
pts = as.matrix(iris[, 3:4])

iters = 2
K = 3
km = kmed = vector("list", iters)
centers = kmed.centers = vector("list", iters + 1)

ind = sample.int(nrow(pts), K)
centers[[1]] = kmed.centers[[1]] = pts[c(99,93,91), , drop = FALSE]

par(mar = c(4,4,3,0))
plot(pts, pch = 19, col = "gray70")
points(centers[[1]][, 1], centers[[1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
title(main = "Iteration 0", line = 2)
title(main = "Choose K arbitrary observations to be the initial cluster centers", line = 1, cex.main = 0.8)

par(mfrow = c(1, 2))
for (i in 1:iters) {
  cat("  \n##", "K-means vs. K-medoids  \n  \\addtocounter{framenumber}{-1}  \n")
  # kmeans
  km[[i]] = kmeans(pts, centers = centers[[i]], nstart = 1, algorithm = "Lloyd", iter.max = 1)
  col = km[[i]]$cluster
  # kmedoids
  kmed[[i]] = kclust(pts, centers = kmed.centers[[i]], alg = "kmedoids", maxiter = 1)
  col.kmed = kmed[[i]]$cluster
  
  # kmeans
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  title(main = paste("K-means iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(centers[[i]][, 1], centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  
  # kmedoids
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col.kmed)
  title(main = paste("K-medoids iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(kmed.centers[[i]][, 1], kmed.centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  
  cat("  \n")
  cat("  \n##", "K-means vs. K-medoids  \n  \\addtocounter{framenumber}{-1}  \n")
  
  centers[[i + 1]] = km[[i]]$centers
  kmed.centers[[i + 1]] = medfromclust(pts, kmed[[i]]$cluster, K)
  
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(centers[[i + 1]][, 1], centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("K-means iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)
  
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(kmed.centers[[i + 1]][, 1], kmed.centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("K-medoids iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)
  # 
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), col = 1:3, pch = 19, cex = 2)
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), pch = 21, cex = 2)
  
  cat("  \n")
}
```

<!-- ## $K$-medoids in R  -->

<!-- - `pam` function of the cluster package.  -->
<!-- - `pamk` function of the fpc package. -->
<!-- - `kmedoids` function of the clue package. -->
<!-- - `KMedoids` function of the TSdist package for clustering time series data.  -->
<!-- - `fviz_nbclust()` function of the factoextra package provides a solution to estimate the optimal number of clusters. -->

## $K$-medoids in R 

The $K$-medoids algorithm is implemented in the `pam` function included in the `cluster` R-package.

We compare $K$-means and $K$-medoids on a modified iris data with additional outliers:

```{r, echo = FALSE, eval = FALSE}
set.seed(5)
random = data.frame(Petal.Length = runif(2, 2.0, 3.5), Petal.Width = runif(2,1.0,10.5))
example = rbind(random, iris[,3:4])
```

```{r, echo = TRUE}
library(cluster)
# add outliers to iris
out = data.frame(Petal.Length = c(2.3, 3), Petal.Width = c(9.7, 3.7))
iris2 = rbind(iris[, 3:4], out)

# K-medoid vs. K-means
kmedoid = pam(iris2, k = 3)
km = kmeans(iris2, centers = 3, iter.max = 100, nstart = 100)
```

## $K$-medoids in R 

```{r, fig.width = 10, fig.height = 6, fig.align="center", out.width="\\textwidth", echo=FALSE}
set.seed(5)
par(mfrow = c(1,2), mar = c(4,3,3,0))

plot(iris2, col=kmedoid$clustering, pch=19, cex=1, main = "K-Medoids")
points(kmedoid$medoids, col="red", pch=4, cex=3, lwd=3)

plot(iris2, col = km$cluster, pch = 19, cex = 1, main = "K-Means")
points(km$centers, col="red", pch=4, cex=3, lwd=3)
```

## Summary

- Minimizing the *within-cluster variation* exactly is not feasible and can be approximated by the $K$-means algorithm.

- $K$-means always converges, however, the cluster assignments strongly depend on the initial
  centers. \
  $\rightarrow$ repeat it several times with different initial centers.

- A simple solution for choosing the number of clusters $K$ is to plot the
  *within-cluster variation* for several $K$ and look for an *"elbow"* which is a good guess for $K$.
  
<!-- - Without the knowledge of the real groups, one can use *indices*, which combine *compactness* and *separation* measures, to validate the clustering assignments. -->


