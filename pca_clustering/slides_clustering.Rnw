% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(MASS)
library(cluster)
library(pdist)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Clustering}
\lecture{Introduction to Machine Learning}
\sloppy

\section{Hierarchical Clustering}

\begin{frame}{Motivation for Clustering}

Consider multivariate data with $N$ observations (e.g. customers) and $P$ 
features (e.g. characteristics of customers).

Task: divide data into groups (clusters), such that 

\begin{itemize}
  \item the observations in each cluster are as "similar" as possible (homogeneity 
  within each cluster), and
  \item the clusters are as "far away" as possible from other clusters (heterogeneity 
  between different clusters).
\end{itemize}

\end{frame}

\begin{frame}{Clustering vs. Classification}

\begin{itemize}
  \item In classification, the groups are known and we try to learn what 
  differentiates these groups (i.e., learn a classification function) to 
  properly classify future data.

  \item In clustering, we look at data, where groups are unknown and try to find 
  similar groups.

\end{itemize}

Why do we need clustering?

  \begin{itemize}
    \item Discovery: looking for new insights in the data (e.g. finding groups of customers that buy a similar product).
    \item Derive a reduced representation of the full data set.
  \end{itemize}
\end{frame}

\begin{frame}{Hierarchical Clustering}

Hierarchical clustering is a recursive process that builds a hierarchy of clusters. 
We distinguish between:

\begin{enumerate}
  \item Agglomerative (or bottom-up) clustering:
  \begin{itemize}
    \item Start: Each observations is an \textit{individual cluster}.
    \item Repeat: Merge the two closest clusters.
    \item Stop when there is only one cluster left.
  \end{itemize}
  \item Divisive (or top-down) clustering:
  \begin{itemize}
    \item Start: All observations are within \textit{one} cluster.
    \item Repeat: Divide the cluster that results in two clusters with biggest distance. 
    \item Stop when each observation is an individual cluster.
  \end{itemize}
\end{enumerate}


\end{frame}

\begin{frame}{Hierarchical Clustering}


---FIX THE TABLE---

Let $X_1,\hdots , X_N$ be observations with $P$ features (dimensions), where 
$X_i = (x_{i1}, \ldots, x_{iP})^\top$. A data set is a (N $\times$ P)-matrix of 
the form:

|        | feature $1$ | $\hdots$   | $\hdots$    | feature $P$|
|:------:|:-----------:|:----------:|:-----------:|:----------:|
|$X_1$   |     $x_{11}$|    $\hdots$|     $\hdots$|    $x_{1P}$|
|$\vdots$|     $\vdots$|    $\vdots$|     $\vdots$|    $\vdots$|
|$X_N$   |     $x_{N1}$|    $\hdots$|     $\hdots$|    $x_{NP}$|
 

\end{frame}

\begin{frame}{Hierarchical Clustering}

For hierarchical clustering, we need a definition for

\begin{itemize}
  \item distances $d(X_i, X_j)$ between two observations $X_i$ and $X_j$:
  \begin{itemize}
    \item manhattan distance: 
      $$d(X_i,X_j)= ||X_i - X_j||_1 = \sum_{k=1}^P|x_{ik}-x_{jk}|$$
    \item euclidean distance:
      $$d(X_i,X_j)= ||X_i - X_j||_2 = \sqrt{\sum_{k=1}^P(x_{ik}-x_{jk})^2}$$
  \end{itemize}
  \item distances between two clusters (called linkage).
\end{itemize}

\end{frame}


\begin{frame}{Distances between Observations}

<<echo=FALSE, results='hide'>>=
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
@

<<echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.5\\textwidth">>=
getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}

par(mar = c(4,4,0,0))
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19, 
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(X[i]), expression(X[j])), adj = c(1.5, 0))
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = 2)
legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(2,1))

text(x = 5, y = 1, expression(d(X[i],X[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = 2)

asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(X[i],X[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
@

\begin{itemize}
  \item \small manhattan: sum up the absolute distances in each dimension.
      In R: \enquote{dist(data, method $=$ "manhattan")}
  \item \small euclidean: remember Pythagoras theorem from school?
      In R: \enquote{dist(data, method $=$ "euclidean")}
  \item \small gower: can be used for mixed variables (categorical and numeric).
      In R: \enquote{gower\_dist()} from the \enquote{gower} package
  \item \small see \enquote{?dist} for other distances.
\end{itemize}

\end{frame}


\begin{frame}{Gower Distance I}

  \begin{itemize}

  \item The Gower's metric calculates the distance between observations $X_i$ and $X_j$ for each feature separately and based on its data type (i.e., categorical or numeric).

  \item For a categorical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by
    $$s_{ijk}=\begin{cases} 0\ \text{if}\ X_{ik}=X_{jk} \\ 1\ \text{if}\ X_{ik}\neq X_{jk}.\end{cases}$$

  \item For a numerical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by $$s_{ijk}=\frac{|X_{ik}-X_{jk}|}{\max(X_k)-\min(X_k)}, \;\;\; \text{so that } 0 \leq s_{ijk} \leq 1.$$ 

  \end{itemize}
\end{frame}

\begin{frame}{Gower Distance II}

The Gower's metric $S$ combines all individual distances of each feature by
$$S_{ij}=\dfrac{\sum_{k=1}^P w_{k} s_{ijk}}{\sum_{k=1}^P w_{k}},$$ where
      
  \begin{itemize}
    \item $P$: number of features.
    \item $w_k$: weight for feature $k$ (typically $w_k = 1$).
    \item $s_{ijk}$: the difference (distance) between $X_{ik}$ and $X_{jk}$, i.e. the $i$-th and $j$-th observation of feature $k$.
  \end{itemize}

\end{frame}
  
\begin{frame}{Gower Distance III - Example}    

\includegraphics[width = \textwidth]{figure_man/gower_distance.png}

\end{frame}

\begin{frame}{Distances between Observations}

It is often a good idea to \textit{normalize} the data before computing distances, 
especially when the scale of features is different, e.g.:

<<echo=FALSE, fig.align="center", fig.height=4, fig.width=9>>=
par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0))

dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)", 
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))


dat$height = dat$height/100
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)", 
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))
@

On the right plot, the distance is dominated by `shoe size`.

\end{frame}


\begin{frame}{Distances between Observations}

\small
The normalized feature $\tilde{X}_{\texttt{height}}$ is computed using 
$X_{\texttt{height}}$ by <!-- Normalization of the \texttt{height} feature means: -->
\[\tilde{X}_{\texttt{height}} = \tfrac{X_{\texttt{height}}-\texttt{mean}(X_{\texttt{height}})}{\texttt{sd}(X_{\texttt{height}})}.\]

\lz

Distances based on normalized data are better comparable and robust in terms of 
linear transformations (e.g. unit conversion).

<<echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.55\\textwidth">>=
par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0))

#dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
dat = as.data.frame(scale(dat))
plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height", 
  pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))), 
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))
@

\end{frame}

\begin{frame}{ Distances between Clusters (Linkage)}

\begin{itemize}
\item Assume that all observations $X_1,\hdots , X_N$ belong to $K<N$ different clusters.

\item The linkage of two clusters $C_r$ and $C_s$ is a "score" describing their distance.
\end{itemize}


The most popular and simplest linkages are

\begin{itemize}
  
\item \textit{Single Linkage} 

\item \textit{Complete Linkage}

\item \textit{Average Linkage} 

\item \textit{Centroid Linkage}

\end{itemize}

<<echo=FALSE, message=FALSE>>=
set.seed(12345)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,2), Sigma = diag(2))
cl2 = mvrnorm(n = n, mu = c(1,-3), Sigma = diag(2)*2)
e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
dat = rbind(cl1, cl2)
d = as.matrix(pdist(cl1, cl2))
@


\end{frame}

\begin{frame}{Single Linkage}

<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth">>=
single = which(d==min(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[single[1],], cl2[single[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
@

Single linkage defines the distance of the \textit{closest point pairs} from different clusters as the distance between two clusters:

\[d_{\text{single}}(C_r,C_s) = \min_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]


\end{frame}

\begin{frame}{Complete Linkage}

<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth">>=
complete = which(d==max(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[complete[1],], cl2[complete[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
@

Complete linkage defines the distance of the *furthest point pairs* of different clusters as 
the distance between two clusters:

\[d_{\text{complete}}(C_r,C_s) = \max_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]


\end{frame}

\begin{frame}{Average Linkage}

<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth">>=
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

for(i in 1:nrow(cl2)) lines(rbind(cl1[single[1],], cl2[i,]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
@

\begin{center}
{\scriptsize (Note: Plot only shows distances between all green points and 
\textit{one} red point)}
\end{center}

In average linkage, the distance between two clusters is defined as the average 
distance across \textit{all} pairs of two different clusters.

\end{frame}

\begin{frame}{Centroid Linkage}

<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.65\\textwidth">>=
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)#, adj = c(0, 0.5))
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)#, adj = c(1, 0.5))


#points(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), pch = 1, cex = 4)
lines(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), type = "b", cex = 3.5)
@

\small
Centroid linkage defines the distance between two clusters as the distance between the two cluster centroids.
The centroid of a cluster $C_s$ with $N_s$ points is the mean value of each dimension:

\[\bar{X}_s = \frac{1}{N_s} \sum_{i \in C_s} X_i\]

\end{frame}

\begin{frame}{Example: Hierarchical Clustering}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$ \\
% Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$\\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
@

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Example: Hierarchical Clustering}

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$ \\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
@

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Example: Hierarchical Clustering}

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{red}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
lines(simple.data[4:5,], col = "red")
@

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Example: Hierarchical Clustering}

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: ${\color{blue}\{1,2,3\}},{\color{red}\{4,5\}}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1:3, 1),], col = "blue")
lines(simple.data[4:5,], col = "red")
@

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Example: Hierarchical Clustering}

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: $\{1,2,3\},\{4,5\}$\\
Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1,2,3,5,4,1),], col = "blue")
@

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Dendrogram}

A Dendrogram is a tree showing which clusters / observations are merged after each step.
The `height' is proportional to the distance between the two merged clusters: 

<<echo = FALSE, results='hide'>>=
# compute distance matrix
euclid = dist(simple.data, method = "euclidean")
# do clustering with complete linkage
agglo = hclust(euclid, method = "complete")
agglo
@

<<echo=FALSE, out.width="\\textwidth", fig.width=8, fig.height=4>>=
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))

par(mar = c(2,3,1,4))
plot(agglo, hang = -1, ylab = "", axes = F)
axis(2, line = 0, padj = 0.5)
title(ylab = "Height", line = 2)
abline(h = c(0, agglo$height), lty = 2, col = "gray70")
text(x = 5, y = c(0, agglo$height), labels = paste("Step ", 1:5), xpd = TRUE, col = "gray70", pos = 4)
@

\end{frame}

\begin{frame}{R Example with Iris Data}

The data contains 150 leaf measurements for 3 flower species:

<<eval = FALSE>>=
pairs(iris[1:4], col = iris$Species)
@

<<echo=FALSE, out.width="0.9\\textwidth", fig.align="center", fig.height = 5.5, fig.width=9>>=
pairs(iris[1:4], pch = 19, col = iris$Species, oma = c(2,2,2,12))
legend(0.95, 1, levels(iris$Species), xpd = TRUE, horiz = F, xjust = 0.5,
       fill = unique(iris$Species), title = "Species")
@
\end{frame}


\begin{frame}{R Example with Iris Data}

We now "forget" the real groups specified by the `Species` variable and try to find clusters based on the leaf measurements. 

<<echo=FALSE, out.width="0.8\\textwidth", fig.align="center">>=
pairs(iris[1:4], pch = 19)
@

\end{frame}

\begin{frame}{R Example with Iris Data}

<<eval=FALSE>>=
# compute distance matrix
d.euclid = dist(iris[1:4], method = "euclidean")
# do clustering with average linkage
cl = hclust(d.euclid, method = "average")
plot(cl, labels = FALSE, hang = -1) # plot dendrogram
rect.hclust(cl, k = 3) # highlight the k = 3 groups
@

<<echo=FALSE, fig.width=12, fig.height=5>>=
par(mar = c(1,4,1,0))
d.euclid = dist(iris[1:4], method = "euclidean")
cl = hclust(d.euclid, method = "average")
plot(cl, labels = FALSE, hang = -1)
rect.hclust(cl, k = 3)
@

\end{frame}

\begin{frame}{R Example with Iris Data}

We can extract the clustering assignments by cutting the dendrogram, e.g. using $k=3$ clusters:

<<eval = FALSE>>=
group = cutree(cl, k = 3) # get clusters assignments for k=3
pairs(iris[1:4], col = group) # plot clusters with different colors
@

<<out.width="0.75\\textwidth", fig.width=8, fig.height=5, fig.align="center", echo = FALSE>>=
group = cutree(cl, k = 3) 
pairs(iris[1:4], pch = 19, col = group) 
@

\end{frame}

\begin{frame}{Properties: Single Linkage}

<<echo=FALSE>>=
#Colors = colorspace::rainbow_hcl(3)
set.seed(0)
x = rbind(
  scale(matrix(rnorm(2*20, sd = 0.4), ncol = 2), center = c(1,1), scale = F),
  scale(matrix(rnorm(2*30, sd = 0.4), ncol = 2), center = -c(1,1), scale = F)
  )
x = rbind(x, matrix(rep(seq(-1,1, length.out = 10), each = 2), ncol = 2, byrow = TRUE))
#x = rbind(x, matrix(runif(2*10, min(x), max(x)), ncol=2))
d = dist(x)

tree.sing = hclust(d,method = "single")
tree.comp = hclust(d,method = "complete")
tree.avg = hclust(d,method = "average")
tree.cen = hclust(d,method = "centroid")

k = 3

labs.sing = cutree(tree.sing, k = k)
labs.comp = cutree(tree.comp, k = k)
labs.avg = cutree(tree.avg, k = k)
labs.cen = cutree(tree.cen, k = k)
@

Single linkage introduces the \textit{chaining problem}:

\begin{itemize}
  \item Only one pair of points needs to be close to merge clusters.
  \item A chain of points can expand a cluster over long distances.
  \item Points within a cluster can be too widely spread and not dense enough.
\end{itemize}

<<echo = FALSE, fig.height=4, out.width="0.7\\textwidth">>=
par(mfrow=c(1,2), mar = c(3,3,2,2))

plot(x,col=Colors[labs.sing], pch = 19, ylab = "", xlab = "", main = "single (chaining effect)")
plot(tree.sing, labels=F, hang=-1)
rect.hclust(tree.sing, k = k)#, border = Colors[1])
@

\end{frame}

\begin{frame}{Properties: Complete Linkage}

Complete linkage avoids chaining, but suffers from \textit{crowding}: 

\begin{itemize}
  \item Merging is based on the furthest distance of point pairs from different clusters.
  \item Points of two different clusters can thus be closer than points within a cluster. 
  \item Clusters are dense, but too close to each other. <!-- and sensitive to outliers. -->
\end{itemize}

<<echo = FALSE, fig.height=4, out.width="0.7\\textwidth">>=
par(mfrow=c(1,2), mar = c(3,3,2,2))

plot(x,col=Colors[labs.comp], pch = 19, ylab = "", xlab = "", main = "complete (crowding effect)")
plot(tree.comp, labels=F, hang=-1)
rect.hclust(tree.comp, k = k)#, border = Colors[1])
@

\end{frame}

\begin{frame}{Properties: Average Linkage}

\begin{itemize}
  \item Average linkage is based on the average distance between clusters and tries to avoid crowding and chaining.
  \item Produces clusters that are quite dense and rather far apart.
\end{itemize}

<<echo=FALSE, fig.height=4.25, fig.width=7, out.width="0.8\\textwidth">>=
par(mfrow=c(2,3), mar = c(3,3,2,2))

plot(x,col=Colors[labs.sing], pch = 19, ylab = "", xlab = "", main = "single (chaining effect)")
plot(x,col=Colors[labs.comp], pch = 19, ylab = "", xlab = "", main = "complete (crowding effect)")
plot(x,col=Colors[labs.avg], pch = 19, ylab = "", xlab = "", main = "average")
#plot(x,col=Colors[labs.cen], pch = 19, ylab = "", xlab = "", main = "centroid")

par(mar = c(1,4,2,1))
plot(tree.sing, labels=F, hang=-1)
rect.hclust(tree.sing, k = k)#, border = Colors[1])
plot(tree.comp, labels=F, hang=-1)
rect.hclust(tree.comp, k = k)#, border = Colors[1])
plot(tree.avg, labels=F, hang=-1)
rect.hclust(tree.avg, k = k)#, border = Colors[1])
#plot(tree.cen, labels=F, hang=-1)
#rect.hclust(tree.cen, k = k)#, border = Colors[1])
@

\end{frame}

\begin{frame}{Properties: Centroid Linkage}

\begin{itemize}
  \item Centroid linkage defines the distance based on \textbf{artificial data points} (the cluster centers), which produces dendrograms \textbf{with inversions}, i.e., the distance between the clusters to be merged can be smaller in the next step.
  \item In single, complete and average linkage, the distance between the clusters to be merged increases in each step. \
  $\Rightarrow$ always produces dendrograms \textbf{without inversions}.
\end{itemize}

<<echo=FALSE, fig.width = 8, fig.height = 3.5, out.width="0.7\\textwidth">>=
simple.data = data.frame(
  x = c(0,1,0.5),
  y = c(0,0,0.9)
)

#simple.data = as.data.frame(rbind(c(1,1), c(5,1), c(3,1+2*sqrt(3))))
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), 
  xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data[1,], labels = 1, pos = 3)
text(simple.data[2,], labels = 2, pos = 3)
text(simple.data[3,], labels = 3, pos = 1)
centroid = colMeans(simple.data[1:2,])
points(centroid[1], centroid[2], col = "gray60", pch = 19)
text(centroid[1], centroid[2], labels = expression(C[12]), col = "gray60", pos = 4)
centroid = colMeans(simple.data[1:3,])
#points(centroid[1], centroid[2], col = "gray60", pch = 19)
#text(centroid[1], centroid[2], labels = expression(C[123]), col = "gray60", pos = 4)

agglo = hclust(dist(simple.data, method = "euclidean")^2, method = "centroid")
#cophenetic(agglo)
agglo$height = sqrt(agglo$height)

par(mar = c(2,4,1,1))
plot(as.dendrogram(agglo), main = "Cluster Dendrogram", ylab = "Height", ylim = c(0,1))
#rect.hclust(agglo, k = 2)
@

\end{frame}

\begin{frame}{Summary}

\begin{itemize}
  \item \textit{Hierarchical agglomerative clustering methods} iteratively merge observations/clusters until all observations are in one single cluster.
  \item Results in a hierarchy of clustering assignments which can be visualized in a \textit{dendrogram}. 
Each node of the dendrogram represents a cluster and its `height' is proportional to the distance of its child nodes.
  \item The most common linkage functions are \textit{single, complete, average} and \textit{centroid} linkage. 
There is no perfect linkage and each linkage has its own advantages and disadvantages.
\end{itemize}

\end{frame}

