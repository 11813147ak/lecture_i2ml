---
output: pdf_document
title: "mlr3 Usecase Day 2 - Ames Housing Dataset"
---
```{r, echo = FALSE}
knitr::opts_chunk$set(echo = solution, eval = solution)
```

This usecase is intended to use `mlr3tuning` to improve the results of the benchmark analysis on the _Ames Dataset_ of day 1.

The main objective of this exercise is as follows:

- To apply an appropriate tuning technique to a learning algorithm in order to obtain improve its predictive performance.
Keep in mind that it takes an appropriate performance measure to evaluate the performance of a tuned algorithm!

## Accessing the dataset

The dataset is available on Kaggle https://www.kaggle.com/prevek18/ames-housing-dataset.
Kaggle is a platform which provides data science competitions and datasets which can be used to get familiar with typical machine learning methods.

## Importing the data

```{r}
housing = read.csv("data/AmesHousing.csv")
housing = mlr::impute(obj = housing, target = "SalePrice",
  classes = list(numeric = mlr::imputeMean(), factor = mlr::imputeMode(), integer = mlr::imputeMedian())
)$data
```

1. Load the `mlr3` and `mlr3learners` packages.

```{r}
library(mlr3)
```

2. Create a regression task object.

```{r}
task = TaskRegr$new(id = "ames_housing", backend = as_data_backend(housing), target = "SalePrice")
# task
```

3. Create a list of learning algorithms which you want to use in the benchmark.

```{r}
# get a featureless learner and a regression tree
library(mlr3learners)
learners = list(
  featureless = lrn("regr.featureless"),
  kknn = lrn("regr.kknn"),
  rpart = lrn("regr.rpart"),
  ranger = lrn("regr.ranger")
)
```

```{r}
library(paradox)
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
tune_ps
```
4. Create a resampling object for your benchmark evaluation.

```{r}
# compare via 3-fold cross validation
resamplings = rsmp("cv", folds = 10)
```


```{r}
library(mlr3tuning)

evals20 = term("evals", 20)

instance = TuningInstance$new(
  task = task,
  learner = learners$rpart,
  resampling = hout,
  measures = measure,
  param_set = tune_ps,
  terminator = evals20
)
print(instance)
```


```{r}
tuner = tnr("grid_search", resolution = 5)
result = tuner$tune(instance)
print(result)
```



```{r}
learners$rpart$param_set$values = result$values
```


```{r}
at = AutoTuner$new(
  learner = learner,
  resampling = resampling,
  measures = measures,
  tune_ps = tune_ps,
  terminator = terminator,
  tuner = tuner
)
at
```

5. Create a grid corresponding to the planned benchmark including the task, all learners and the resampling strategy.

```{r}
# create a BenchmarkDesign object
design = benchmark_grid(task, learners, resamplings)
print(design)
```

6. Run the benchmark

```{r}
# execute the benchmark
bmr = benchmark(design)
```

7. Use appropriate regression measures to measure the performance of each learner in the benchmark.

```{r}
# get some measures: accuracy (acc) and area under the curve (auc)
measures = mlr_measures$mget(c("regr.mse", "regr.mae"))
bmr$aggregate(measures)
```

8. Use an appropriate plot to illustrate the benchmark results.
Have e.g. a look at the `mlr3viz` package.

```{r}
library(mlr3viz)
autoplot(bmr)
```


